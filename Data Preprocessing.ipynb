{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5abde641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "from os.path import isdir\n",
    "import linecache\n",
    "# python自带的模块，缓存读取文件某一行的内容\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import copy as copy1\n",
    "import scipy\n",
    "import urllib\n",
    "# from scipy.signal import savgol_filter\n",
    "import urllib.parse\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "from numpy import loadtxt\n",
    "from numpy import savetxt\n",
    "from numpy.linalg import svd\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "# 可以使数组打印更漂亮\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "# 解决ndarray打印省略问题\n",
    "# np.set_printoptions(linewidth=20)\n",
    "# #打印时候每行显示20个\n",
    "\n",
    "import pylab\n",
    "from pylab import *\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = ['Times New Roman']\n",
    "# ！\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# 用来正常显示负号\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "\n",
    "def to_percent(temp, position):\n",
    "    return '%1.0f' % (100 * temp) + '%'\n",
    "\n",
    "\n",
    "# 刻度以%显示，小数点后1位\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy import signal\n",
    "from scipy import optimize\n",
    "from scipy import sparse\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from scipy.signal import detrend\n",
    "from scipy.sparse import linalg\n",
    "from scipy.sparse import csc_matrix, eye, diags\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy import cluster\n",
    "# 用于进行层次聚类，话层次聚类图的工具包\n",
    "from scipy.spatial.distance import cdist\n",
    "# 计算距离结算公式\n",
    "\n",
    "from collections import Counter\n",
    "# 高性能容量数据类型,count计数器\n",
    "from decimal import *\n",
    "import sklearn\n",
    "from sklearn import decomposition as skldec\n",
    "# 用于主成分分析降维的包\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "# tsne\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_score\n",
    "# 计算轮廓系数\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# 网格搜索&交叉验证\n",
    "#from sklearn.externals import joblib\n",
    "import joblib\n",
    "# 保存、加载分类器模型\n",
    "from xgboost import XGBClassifier\n",
    "# xgboost\n",
    "from skimage import filters\n",
    "# 基于Otsu的阈值分割方法,无参数去背景\n",
    "\n",
    "\n",
    "# ROCq曲线AUC值\n",
    "# from sklearn import datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from scipy import interp\n",
    "# 画图-混淆矩阵\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# -----------------------\n",
    "import struct\n",
    "# Kmeans模型\n",
    "import sklearn.cluster\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# 聚类算法距离计算公式\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# 写入word\n",
    "import docx\n",
    "import docx.enum\n",
    "import docx.shared\n",
    "import docx.enum.text\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
    "import docx.oxml\n",
    "import docx.oxml.ns\n",
    "from docx.oxml.ns import qn\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# 忽略警告\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5a4cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# airPLS去背景\n",
    "def WhittakerSmooth(x, w, lambda_):\n",
    "    X = np.matrix(x)\n",
    "    m = X.size\n",
    "    i = np.arange(0, m)\n",
    "    E = eye(m, format='csc')\n",
    "    D = E[1:] - E[:-1]\n",
    "    # numpy.diff() does not work with sparse matrix. This is a workaround.\n",
    "    W = diags(w, 0, shape=(m, m))\n",
    "    A = csc_matrix(W + (lambda_ * D.T * D))\n",
    "    B = csc_matrix(W * X.T)\n",
    "    background = spsolve(A, B)\n",
    "    return np.array(background)\n",
    "\n",
    "\n",
    "# airPLS去背景主程序\n",
    "def airPLS(x, lambda_, itermax):\n",
    "    x = np.array(x)\n",
    "    m = x.shape[0]  # 计算长宽维度\n",
    "    w = np.ones(m)\n",
    "    for i in range(1, itermax + 1):\n",
    "        z = WhittakerSmooth(x, w, lambda_)\n",
    "        d = x - z\n",
    "        dssn = np.abs(d[d < 0].sum())\n",
    "        # ***********************************************除零问题待解决\n",
    "        # if fabs(dssn) < 1e-15:\n",
    "        #     dssn = dssn+0.0001\n",
    "        # print(\"dssn:\",dssn)\n",
    "        if (dssn < 0.001 * (abs(x)).sum() or i == itermax):\n",
    "            if (i == itermax):\n",
    "                print('WARING max iteration reached!')\n",
    "            break\n",
    "        w[d >= 0] = 0\n",
    "        # d>0 means that this point is part of a peak, so its weight is set to 0 in order to ignore it\n",
    "        w[d < 0] = np.exp(i * np.abs(d[d < 0]) / dssn)\n",
    "        w[0] = np.exp(i * (d[d < 0]).max() / dssn)\n",
    "        w[-1] = w[0]\n",
    "    z = d\n",
    "    return z.tolist()\n",
    "\n",
    "\n",
    "# 无参数调用去背景算法\n",
    "\n",
    "# 方法一，出现部分bug,如最小长度必须大于101等，选用方法二\n",
    "# def background_substruction(data):\n",
    "#     data = np.array(data)\n",
    "#     polyorder = 3\n",
    "#     block_size = 101\n",
    "#     baseline = np.zeros((len(data)))\n",
    "#     # 定义基线\n",
    "#     for x in range(int(len(data))):\n",
    "#         min_half = int(block_size / 2)\n",
    "#         max_half = int(block_size / 2)\n",
    "#         if x - min_half < 0:\n",
    "#             min_half = 0\n",
    "#         if x + max_half > len(data):\n",
    "#             max_half = len(data)\n",
    "#         selected_data = data[x - min_half:x + max_half]\n",
    "#         # print(\"长度：\",type(selected_data))\n",
    "#         thre = filters.threshold_otsu(np.asarray((selected_data)))\n",
    "#         # print(\"阈值：\",thre)\n",
    "#         # test = selected_data\n",
    "#         test = selected_data[selected_data < thre]\n",
    "#         # 剔除小于“thre”阈值的数据点\n",
    "#         # print(\"test1:\",len(test))\n",
    "#         test = test[test < np.mean(test)]\n",
    "#         test = test[test < np.median(test)]\n",
    "#         # print(\"test2:\", len(test))\n",
    "#         # print(\"x\",min(test))\n",
    "#         # baseline[x] = min(test)\n",
    "#         baseline[x] = np.mean(test)\n",
    "#         # print(\"长度：\",len(baseline))\n",
    "#         # print(\"基线：\",baseline)\n",
    "#     smooth_baseline = signal.savgol_filter(baseline, block_size, polyorder)\n",
    "#     corrected_data = data - smooth_baseline\n",
    "#     return corrected_data.tolist()\n",
    "\n",
    "# 方法二，ALS交替最小二乘算法\n",
    "def molification_smoothing(rawspectrum, struct_el, number_of_molifications):\n",
    "    molifier_kernel = np.linspace(-1, 1, num=struct_el)\n",
    "    # print(molifier_kernel)\n",
    "    # 创建-1到1之间数字序列，步长为struct_el\n",
    "    molifier_kernel[1:-1] = np.exp(-1 / (1 - molifier_kernel[1:-1] ** 2))\n",
    "    # print(molifier_kernel)\n",
    "    molifier_kernel[0] = 0\n",
    "    molifier_kernel[-1] = 0\n",
    "    molifier_kernel = molifier_kernel / np.sum(molifier_kernel)\n",
    "    # 列表归一化\n",
    "    denominormtor = np.convolve(np.ones_like(rawspectrum), molifier_kernel, 'same')\n",
    "    # np.convolve卷积运算，same：返回的数组长度为max(M, N),边际效应依旧存在\n",
    "    # np.ones_like，全1数组\n",
    "    smoothline = rawspectrum\n",
    "    i = 0\n",
    "    for i in range(number_of_molifications):\n",
    "        smoothline = np.convolve(smoothline, molifier_kernel, 'same') / denominormtor\n",
    "        i += 1\n",
    "    return smoothline\n",
    "\n",
    "\n",
    "# 方法二，ALS交替最小二乘算法主函数\n",
    "def background_substruction(y):\n",
    "    als_lambda = 3e4\n",
    "    als_p_weight = 5e-4\n",
    "    _iterations = 16\n",
    "    zero_step_struct_el = np.int(2 * np.round(len(y) / 200) + 3)\n",
    "    \"\"\"\n",
    "    猜测，设置步长每10个点为一步，步长越短，拟合效果越保型，最终点数至少大于等于3，所以这里将“+1”该为“+3”\n",
    "    \"\"\"\n",
    "    # print(zero_step_struct_el)\n",
    "    y_sm = molification_smoothing(y, zero_step_struct_el, 16)\n",
    "    # 交替（或迭代）做平滑,平滑效果其实不好，主要为了拟合光谱形状\n",
    "    # plt.plot(y,\"r\")\n",
    "    # plt.plot(y_sm,\"b\")\n",
    "    # plt.show()\n",
    "    # compute the derivatives:\n",
    "    y_sm_1d = np.gradient(y_sm)\n",
    "    # !!!!!!重要理解 https://docs.scipy.org/doc/numpy-1.12.0/reference/generated/numpy.gradient.html\n",
    "    # numpy.gradient，返回 N 维数组的梯度，\n",
    "    # 梯度是使用内部二阶精确的中心差异和边界处的一阶差异或二阶精确的一侧（向前或向后）差异来计算的。\n",
    "    # 因此，返回的梯度与输入数组具有相同的形状。\n",
    "    y_sm_2d = np.gradient(y_sm_1d)\n",
    "    # weighting function for the 2nd der:二阶权函数\n",
    "    y_sm_2d_decay = (np.mean(y_sm_2d ** 2)) ** 0.5\n",
    "    # 二阶权值数组平方后均值在开方\n",
    "    weifunc2D = np.exp(-y_sm_2d ** 2 / 2 / y_sm_2d_decay ** 2)\n",
    "    # weighting function for the 1st der:\n",
    "    y_sm_1d_decay = (np.mean((y_sm_1d - np.mean(y_sm_1d)) ** 2)) ** 0.5\n",
    "    weifunc1D = np.exp(-(y_sm_1d - np.mean(y_sm_1d)) ** 2 / 2 / y_sm_1d_decay ** 2)\n",
    "    weifunc = weifunc1D * weifunc2D\n",
    "    # exclude from screenenig the edges of the spectrum (optional)，排除光谱的边缘\n",
    "    weifunc[0:zero_step_struct_el] = 1\n",
    "    weifunc[-zero_step_struct_el:] = 1\n",
    "\n",
    "    # estimate the peak height，估计峰值高度\n",
    "    peakscreen_amplitude = (np.max(detrend(y)) - np.min(\n",
    "        detrend(y))) / 2\n",
    "    # 尚不理解：8 is good, because this is a characteristic height of a tail\n",
    "    \"\"\"\n",
    "    detrend(y),去除光谱趋势\n",
    "    \"\"\"\n",
    "    L = len(y)\n",
    "    D = sparse.diags([1, -2, 1], [0, -1, -2], shape=(L, L - 2))\n",
    "    D = als_lambda * D.dot(D.transpose())\n",
    "    w = np.ones(L)\n",
    "    W = sparse.spdiags(w, 0, L, L)\n",
    "    # k = 10 * morphological_noise(y)\n",
    "    #  above this height the peaks are rejected\n",
    "    for i in range(_iterations):\n",
    "        W.setdiag(w)\n",
    "        Z = W + D\n",
    "        z = spsolve(Z, w * y)\n",
    "        w = als_p_weight * weifunc * np.exp(-((y - z) / peakscreen_amplitude) ** 2 / 2) * \\\n",
    "            (y > z) + (1 - als_p_weight) * (y < z)\n",
    "    # baseline = z\n",
    "    corrected_data = y - z\n",
    "    return corrected_data.tolist()\n",
    "\n",
    "\n",
    "# experimental_spectrum = np.genfromtxt\\\n",
    "#     (r'C:\\Users\\lixin\\Desktop\\测试数据\\5种分类\\#01\\1-1200g-2-10s_1.txt')\n",
    "# x = experimental_spectrum[:, 0]\n",
    "# exp_y = experimental_spectrum[:, 1]\n",
    "# bldp = background_substruction(exp_y)\n",
    "#\n",
    "# plt.figure()\n",
    "# plt.plot(x, exp_y, 'k', label='exp.spec', LineWidth=1)\n",
    "# plt.plot(x, exp_y - bldp, 'r', label='derpsalsa', LineWidth=1)\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# 1读取文件夹并合并所有文件\n",
    "# 判断字符串是否为数字\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "# 合并文件夹，分类\n",
    "def readMerge(dataPath):\n",
    "    root = dataPath\n",
    "    # file_names = os.listdir(root)\n",
    "    file_names = []\n",
    "    # 存储所有txt文档文件名\n",
    "    f_list = os.listdir(root)\n",
    "    # print(\"&1\",f_list)\n",
    "    # f_list.sort(key= lambda x:int(x[-6:-4]))\n",
    "    # print(\"&2\", f_list)\n",
    "    f_list.sort()\n",
    "    # 排序\n",
    "    # print (\"**\",f_list)\n",
    "    for i in f_list:\n",
    "        # os.path.splitext():分离文件名与扩展名\n",
    "        if os.path.splitext(i)[1] == '.txt':\n",
    "            # 判断是否为txt文档，\n",
    "            file_names.append(i)\n",
    "\n",
    "    file_ob_list = []\n",
    "    for file_name in file_names:\n",
    "        # 循环地给这520个文件名加上它前面的路径，以得到它的具体路径\n",
    "        fileob = root + '/' + file_name\n",
    "        # 文件夹路径加上/ 再加上具体要读的的txt的文件名就定位到了这个txt\n",
    "        file_ob_list.append(fileob)\n",
    "    # temp = np.loadtxt(file_ob_list[0])\n",
    "    # allDataSpe = np.array(temp)[:, 0]\n",
    "    # for fileNameInter in file_ob_list:\n",
    "    #     temp = np.loadtxt(file_ob_list[0])\n",
    "    #     allDataSpe = np.column_stack((allDataSpe, np.array(temp)[:, 1]))\n",
    "    # print(\"11:\",file_ob_list)\n",
    "    firLine = np.array(pd.read_csv(file_ob_list[0], delim_whitespace=True, header=None))[0]\n",
    "    # 存储首行内容\n",
    "    if (is_number(firLine[0]) and is_number(firLine[1])):\n",
    "        # 首行数字\n",
    "        # print(\"首行数字\")\n",
    "        temp = pd.read_csv(file_ob_list[0], delim_whitespace=True, header=None)\n",
    "        allDataSpe = np.array(temp)[:, 0]\n",
    "        # allDataSpe = np.column_stack((allDataSpe, allDataSpe1))\n",
    "        # print(\"**\",allDataSpe.shape)\n",
    "        # return 0\n",
    "        # COUNT = 0\n",
    "        # 用于查找编码格式错误的问题\n",
    "        for fileNameInter in file_ob_list:\n",
    "            # print(\"?\",fileNameInter)\n",
    "            # 打印出当前数据路径\n",
    "            temp = pd.read_csv(fileNameInter, delim_whitespace=True, header=None, encoding='')\n",
    "            # print(\"##\", COUNT)\n",
    "            allDataSpe = np.column_stack((allDataSpe, np.array(temp)[:, 1]))\n",
    "            # COUNT = COUNT+1\n",
    "            # print(\"**\",COUNT)\n",
    "            # print(\"**\",allDataSpe.shape)\n",
    "    else:\n",
    "        # 首行非数字\n",
    "        # print(\"首行非数字\")\n",
    "        temp = pd.read_csv(file_ob_list[0], delim_whitespace=True)\n",
    "        allDataSpe = np.array(temp)[:, 0]\n",
    "        for fileNameInter in file_ob_list:\n",
    "            temp = pd.read_csv(fileNameInter, delim_whitespace=True)\n",
    "            allDataSpe = np.column_stack((allDataSpe, np.array(temp)[:, 1]))\n",
    "    # print(allDataSpe.shape)\n",
    "    return np.array(allDataSpe, dtype=np.float64)\n",
    "\n",
    "    #\n",
    "\n",
    "\n",
    "# 一个文件夹下聚类，记住小翟的id标签号\n",
    "# 判断字符串是否为数字\n",
    "# def is_number(s):\n",
    "#     try:\n",
    "#         float(s)\n",
    "#         return True\n",
    "#     except ValueError:\n",
    "#         pass\n",
    "#\n",
    "#     try:\n",
    "#         import unicodedata\n",
    "#         unicodedata.numeric(s)\n",
    "#         return True\n",
    "#     except (TypeError, ValueError):\n",
    "#         pass\n",
    "#\n",
    "#     return False\n",
    "# 聚类\n",
    "def readMerge1(dataPath):\n",
    "    root = dataPath\n",
    "    # file_names = os.listdir(root)\n",
    "    file_names = []\n",
    "    # 存储所有txt文档文件名\n",
    "    f_list = os.listdir(root)\n",
    "    # print f_list\n",
    "    for i in f_list:\n",
    "        # os.path.splitext():分离文件名与扩展名\n",
    "        if os.path.splitext(i)[1] == '.txt':\n",
    "            # 判断是否为txt文档，\n",
    "            file_names.append(i)\n",
    "\n",
    "    file_ob_list = []\n",
    "    idLabel = []\n",
    "    # 用于存储小翟的id标签号\n",
    "    for file_name in file_names:\n",
    "        # 循环地给这520个文件名加上它前面的路径，以得到它的具体路径\n",
    "        fileob = root + '/' + file_name\n",
    "\n",
    "        # 识别标签，保存文件名\n",
    "        if (len(file_name) < 12):\n",
    "            idLabel.append(file_name[:-4])\n",
    "        else:\n",
    "            # 自动选择,命名规则“#001_001.txt”\n",
    "            if (file_name[-12] == '#'):\n",
    "                idLabel.append(file_name[-12: -4])\n",
    "            else:\n",
    "                idLabel.append(file_name[:-4])\n",
    "\n",
    "        file_ob_list.append(fileob)\n",
    "    # print(idLabel)\n",
    "    # return 0\n",
    "    firLine = np.array(pd.read_csv(file_ob_list[0], delim_whitespace=True, header=None, encoding='utf-8'))[0]\n",
    "    # 存储首行内容\n",
    "    if (is_number(firLine[0]) and is_number(firLine[1])):\n",
    "        # 首行数字\n",
    "        # print(\"首行数字\")\n",
    "        temp = pd.read_csv(file_ob_list[0], delim_whitespace=True, header=None)\n",
    "        # print(np.array(temp).shape)\n",
    "        # return 0\n",
    "        allDataSpe = np.array(temp)[:, 0]\n",
    "        '''\n",
    "        # 实现单个数据截谱1\n",
    "        if np.array(allDataSpe).shape[0] == 1340:\n",
    "            allDataSpe = allDataSpe[110 :]\n",
    "        '''\n",
    "        # allDataSpe = np.column_stack((allDataSpe, allDataSpe1))\n",
    "        # print(allDataSpe.shape)\n",
    "        # return 0\n",
    "        for fileNameInter in file_ob_list:\n",
    "            temp = pd.read_csv(fileNameInter, delim_whitespace=True, header=None)\n",
    "            '''\n",
    "            # 实现单个数据截谱2\n",
    "            temp = np.array(temp)\n",
    "            print(temp.shape[0])\n",
    "            if np.array(temp).shape[0] == 1340:\n",
    "                temp = temp[110:,:]\n",
    "            '''\n",
    "            # print(\"1\",temp.shape[0])\n",
    "            allDataSpe = np.column_stack((allDataSpe, np.array(temp)[:, 1]))\n",
    "    else:\n",
    "        # 首行非数字\n",
    "        # print(\"首行非数字\")\n",
    "        temp = pd.read_csv(file_ob_list[0], delim_whitespace=True)\n",
    "        allDataSpe = np.array(temp)[:, 0]\n",
    "        for fileNameInter in file_ob_list:\n",
    "            temp = pd.read_csv(fileNameInter, delim_whitespace=True)\n",
    "            allDataSpe = np.column_stack((allDataSpe, np.array(temp)[:, 1]))\n",
    "\n",
    "    # data = []\n",
    "    return np.array(allDataSpe, dtype=np.float64), idLabel\n",
    "    # 返回ldata将list型转为numpy，方便索引\n",
    "\n",
    "\n",
    "# 2.1去除宇宙射线\n",
    "# def Norris(y1,n):\n",
    "#     y = np.diff(y1)\n",
    "#     Y = copy1.copy(y1)\n",
    "#     temp = (max(y1) - min(y1)) / n\n",
    "#     # list\n",
    "#     for i in range(1, len(y) - 3):\n",
    "#         if (abs(y[i]) > temp) :\n",
    "#             if(y[i]>0):\n",
    "#                 Y[i+1] = Y[i]\n",
    "#             else:\n",
    "#                 Y[i+1] = Y[i+2]\n",
    "#     return Y\n",
    "def CRR(y, filterSize, dynamicFactor):\n",
    "    N1 = len(y)\n",
    "    # print(N)\n",
    "    crrDot = []\n",
    "    for i in range(filterSize, N1 - filterSize - 1):\n",
    "        # maxY = y[i - filterSize:i + filterSize + 1].max()\n",
    "        minY = y[i - filterSize:i + filterSize + 1].min()\n",
    "        medianY = np.median(y[i - filterSize:i + filterSize + 1])\n",
    "        if ((y[i] - minY) > (medianY - minY) * dynamicFactor):\n",
    "            crrDot.append(i)\n",
    "    N2 = len(crrDot)\n",
    "    if N2 == 0:\n",
    "        return y\n",
    "    else:\n",
    "        for j in range(N2):\n",
    "            count = 0\n",
    "            while (((j + count + 1) < N2) and ((crrDot[j + count + 1] - crrDot[j + count]) == 1)):\n",
    "                count = count + 1\n",
    "            x1 = crrDot[j] - 1\n",
    "            x2 = crrDot[j + count] + 1\n",
    "            y1 = y[x1]\n",
    "            y2 = y[x2]\n",
    "            y[crrDot[j]] = (y2 - y1) * (crrDot[j] - x1) / (x2 - x1) + y1\n",
    "        return y\n",
    "\n",
    "\n",
    "# 2.2 S-G滤波\n",
    "# from scipy.signal import savgol_filter\n",
    "\n",
    "# 2.3 airPLS基线扣除\n",
    "\n",
    "# 见上文算法\n",
    "\n",
    "# 2.4 归一化\n",
    "\n",
    "def MaxMinNormalization(x):\n",
    "    Max = np.max(x)\n",
    "    Min = np.min(x)\n",
    "    x = (x - Min) / (Max - Min);\n",
    "    # y=x.tolist()\n",
    "    return x\n",
    "\n",
    "\n",
    "# 删除文件夹下所有tif图片\n",
    "def del_files(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for name in files:\n",
    "            if name.endswith(\".tif\"):\n",
    "                os.remove(os.path.join(root, name))\n",
    "    return 0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a3ca0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **************************************************#################################***********************************#############################*******************************一键生成报告\n",
    "\n",
    "def oneclickReport(dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize,\n",
    "                   dynamicFactor, winSG, nSG, lambdaAirPls,\n",
    "                   itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize, pcaNcomponents, pca_col1,\n",
    "                   pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter, nGrad, svmTestSize, svmC, xKernel,\n",
    "                   clusters_N):\n",
    "    # dataPath:数据文件路径\n",
    "    # startSpec:截谱起始删除波数\n",
    "    # endSpec：截谱尾删除波数\n",
    "    # nNorris：宇宙射线阈值\n",
    "    # winSG:SG滤波窗口\n",
    "    # nSG:SG滤波阶次\n",
    "    # lambdaAirPls:基线校正\n",
    "    # itermaxAirPls:基线校正迭代次数\n",
    "\n",
    "    # #将传入参数写入txt\n",
    "    # listTable2Name = [\"dataPath\", \"outPath\", \"startSpec\", \"endSpec\", \"snrNoiseindex\", \"delSNRmin\", \"delSNRmax\",\n",
    "    #                   \"filterSize\",\n",
    "    #                   \"dynamicFactor\", \"winSG\", \"nSG\", \"lambdaAirPls\", \"itermaxAirPls\", \"pltInter\", \"processLinewidth\",\n",
    "    #                   \"distanceMethod\", \"hcaLabelsize\", \"pcaNcomponents\", \"pca_col1\", \"pca_col2\", \"pcaSize\", \"pcaAlpha\",\n",
    "    #                   \"nDim\", \"nPerplex\", \"learnRate\", \"nInter\", \"nGrad\", \"svmTestSize\", \"svmC\", \"xKernel\",\n",
    "    #                   \"clusters_N\"]\n",
    "    # listTable2Meaning = [\"输入路径\", \"输出路径\", \"截谱长度(首)\", \"截谱长度(尾)\", \"静默区：尾端长度\", \"信噪比最小阈值\", \"信噪比最大阈值\",\n",
    "    #                      \"宇宙射线：窗口宽度\", \"宇宙射线：动态因子\", \"滤波：窗口宽度\", \"滤波：拟合阶次\",\n",
    "    #                      \"基线校正：lambda\", \"基线校正：迭代次数\", \"显示光谱量(未使用)\", \"光谱线宽(未使用)\",\n",
    "    #                      \"HCA距离度量方法:\\r0:沃德方差最小化\\r1:质心欧式距离\\r2:加权分组平均\\r3:UPGMA算法\",\n",
    "    #                      \"HCA(tSNE)标号大小\", \"PCA主成分个数\", \"PCA(tSNE)选择维度\", \"PCA(tSNE)选择维度\",\n",
    "    #                      \"PCA(tSNE)标签大小\", \"PCA(tSNE)透明度\", \"tSNE空间维度\", \"Perplex\", \"tSNE学习率\",\n",
    "    #                      \"tSNE最大迭代次数\", \"tSNE收敛误差\", \"测试数据占比\", \"SVM错误项惩罚系数\",\n",
    "    #                      \"SVM核函数:\\r0:多项式核\\r1:线性核\\r2:径向基核\\r3:sigmod核\\r4:核矩阵\", \"聚类堆数\"]\n",
    "    # listTable2Value = [dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize,\n",
    "    #                    dynamicFactor, winSG,\n",
    "    #                    nSG, lambdaAirPls, itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize,\n",
    "    #                    pcaNcomponents, pca_col1, pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter, nGrad,\n",
    "    #                    svmTestSize, svmC, xKernel, clusters_N]\n",
    "    #\n",
    "    listTable2Value = [dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize,\n",
    "                       dynamicFactor, winSG,\n",
    "                       nSG, lambdaAirPls, itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize,\n",
    "                       pcaNcomponents, pca_col1, pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter,\n",
    "                       nGrad, svmTestSize, svmC, xKernel, clusters_N]\n",
    "    listTable2Name = [\"dataPath\", \"outPath\", \"startSpec\", \"endSpec\", \"snrNoiseindex\", \"delSNRmin\", \"delSNRmax\",\n",
    "                      \"filterSize\",\n",
    "                      \"dynamicFactor\", \"winSG\", \"nSG\", \"lambdaAirPls\", \"itermaxAirPls\", \"pltInter\",\n",
    "                      \"processLinewidth\",\n",
    "                      \"distanceMethod\", \"hcaLabelsize\", \"pcaNcomponents\", \"pca_col1\", \"pca_col2\", \"pcaSize\",\n",
    "                      \"pcaAlpha\",\n",
    "                      \"nDim\", \"nPerplex\", \"learnRate\", \"nInter\", \"nGrad\", \"svmTestSize\", \"svmC\", \"xKernel\",\n",
    "                      \"clusters_N\"]\n",
    "\n",
    "    f = open(outPath + '/' + r'parameter.txt', \"w\")\n",
    "    for i in range(len(listTable2Value)):\n",
    "        f.write(listTable2Name[i] + ':')\n",
    "        f.write(str(listTable2Value[i]) + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "    # 定义颜色和形状，使用取余运算做循环\n",
    "    # colorSpectrumColour = ['#1C1C1C','#008B8B', '#006400', '#00FFFF', '#228B22', '#90EE90', '#8B8B00', '#CD5C5C',\n",
    "    #                        '#698B22', '#FFA500', '#9932CC', '#3A5FCD', '#5D478B']\n",
    "    #colorSpectrumColour = ['#FF7F00', '#8B0000', '#FFFF00', '#00008B', '#006400', '#90EE90', '#698B69',\n",
    "    #                       '#00FFFF', '#008B45', '#9932CC', '#3A5FCD', '#5D478B', '#1C1C1C']\n",
    "    colorSpectrumColour = ['#8A9FC8', '#FB8C61', '#FFFF00', '#00008B', '#006400', '#90EE90', '#698B69',\n",
    "                           '#00FFFF', '#008B45', '#9932CC', '#3A5FCD', '#5D478B', '#1C1C1C']\n",
    "    \n",
    "    # colorSpectrumColour = ['#90EE90', '#8B0000', '#8B008B', '#008B8B', '#00008B', '#698B69', '#FFE1FF', '#FFA500',\n",
    "    #                        '#00FFFF', '#008B45', '#9932CC', '#3A5FCD', '#5D478B']\n",
    "    # 光谱蓝色颜色\n",
    "    colorSpectrumBlue = ['0000FF', '0000EE', '0000DD', '0000CC', '0000BB', '0000AA', '000099', '000088', '000077',\n",
    "                         '000066', 'BCBCBC', 'CCCCCC', 'DCDCDC', 'ECECEC']\n",
    "    # 光谱渐变灰色\n",
    "    colorSpectrumGrey = ['#1C1C1C', '#2C2C2C', '3C3C3C', '4C4C4C', '5C5C5C', '6C6C6C', '7C7C7C', '8C8C8C', '9C9C9C',\n",
    "                         'ACACAC', 'BCBCBC', 'CCCCCC', 'DCDCDC', 'ECECEC']\n",
    "    colors = ['#1C1C1C', '#0000FF', '#FFFF00', '#7FFFD4', '#008B45']\n",
    "    # markers = [\"s\", \"o\", \"+\", \"v\", \"d\", \"x\", \"3\", \"p\", \".\", \"*\", \"1\",\"<\",\">\",\",\"]\n",
    "    # \"正方形\"，\"圈\"，\"加号\"，\"倒三角\"，\"细钻\"，\"×号\"，\"tri_left\",\"五角\"，“点”，“tri_down”,\"左三角形\",\"右三角形\",\"像素\"\n",
    "    markers = [\"s\", '^', \"o\", \"+\", \"v\", \"d\", \"x\", \"*\", \"h\", \"3\", \"p\", \".\", \"1\", \"<\", \">\", \"D\"]\n",
    "    # (1)□(2)△(3)○(4)＋(5)▽(6)♢(7)×(8)☆(9)“正六边形”(10)“左Y”(11)“正五边形”(12)·(13)“正Y”(14)◁(15)▷(16)◇\n",
    "    files = os.listdir(dataPath)\n",
    "    # 返回指定的文件夹包含的文件或文件夹的名字的列表\n",
    "    # ********************************************************************************************************************************无文件夹\n",
    "    if len(files) == 0:\n",
    "        return 0\n",
    "    # *********************************************************************************************************************************一个文件夹\n",
    "    elif len(files) == 1:\n",
    "        # *******************************************************************************************将该文件夹下数据合并\n",
    "        dataPathOne = dataPath + \"/\" + files[0]\n",
    "        # print(\"路径：\",datapath1)\n",
    "        dataFileOne, idLabel = readMerge1(dataPathOne)\n",
    "        # print(\"合并后数据\",dataOneFile.shape)\n",
    "        # idLabel:存储映射后的id标签，用于后续返回给洪姐\n",
    "        idLabelAll = copy1.copy(idLabel)\n",
    "        # return 0\n",
    "        # **********************************************************************************************************截谱\n",
    "        dataFileOne = dataFileOne[startSpec: (dataFileOne.shape[0] - endSpec), :]\n",
    "\n",
    "        # print(\"合并后数据\",dataOneFile.shape)\n",
    "        dataFileOne1 = copy1.copy(dataFileOne)\n",
    "        # 注意：直接采用 ’ = ’ 赋值方式会使得两个变量占用同一地址\n",
    "        x = dataFileOne[:, 0]\n",
    "        snrFileOne = []\n",
    "        # 存储拉曼光谱信噪比\n",
    "        snrDelfileOne = []\n",
    "        # 存储删除信噪比小于或大于阈值的光谱\n",
    "        idLabelOneDel = []\n",
    "        # #存储删除信噪比光谱的小翟映射ID\n",
    "        idLabelOne = []\n",
    "        countDelSpect = 0\n",
    "        # 用于统计被删除的光谱数量\n",
    "        meanSpectrumOne = []\n",
    "        # 光谱均值\n",
    "        stdSpectrumOne = []\n",
    "        # 光谱方差\n",
    "\n",
    "        for i in range(1, dataFileOne.shape[1]):\n",
    "            snrS = 0\n",
    "            # 初始化该光谱信噪比\n",
    "            # *****************************************************************************************循环处理每个光谱\n",
    "            # idLabel.append(i)\n",
    "            # 如果不识别标签使用该语句，idLabel用ID代替\n",
    "            y = dataFileOne[:, i]\n",
    "            # 从第二列开始，保留了第一列坐标，后面需要去掉\n",
    "            y1 = CRR(y, filterSize, dynamicFactor)\n",
    "            y2 = savgol_filter(y1, winSG, nSG)\n",
    "            # 2.3 airPLS\n",
    "            y3 = airPLS(y2, lambdaAirPls, itermaxAirPls)\n",
    "            # 有参去背景（自适应迭代重加权惩罚最小二乘，airpls）\n",
    "            # y3 = background_substruction(y2)\n",
    "            # 无参去背景（交替最小二乘ALS）\n",
    "            # 2.4 归一化\n",
    "            y4 = MaxMinNormalization(y3)\n",
    "            # ********************************************************************************************信噪比计算应用\n",
    "            snrSpectrumSingal = max(y4) - min(y4)\n",
    "            # 信号强度\n",
    "            snrSpectrumNoise = max(y4[-snrNoiseindex:]) - min(y4[-snrNoiseindex:])\n",
    "            # 噪声强度\n",
    "            snrS = snrSpectrumSingal / snrSpectrumNoise\n",
    "            # 光谱信噪比\n",
    "            if snrS < delSNRmin or snrS > delSNRmax:\n",
    "                snrDelfileOne.append(i)\n",
    "                # 存储删除实际标签\n",
    "                idLabelOneDel.append(idLabel[i - 1])\n",
    "                # # 存储删除小翟映射标签\n",
    "                countDelSpect = countDelSpect + 1\n",
    "                continue\n",
    "\n",
    "            snrFileOne.append(snrS)\n",
    "            # 信噪比拼接\n",
    "            dataFileOne[:, i - countDelSpect] = y4\n",
    "            idLabelOne.append(idLabel[i - 1])\n",
    "            # 删除后剩余标签映射\n",
    "        # ***************************************************************************************************************均值方差图\n",
    "        figure()\n",
    "        dataFileOne = dataFileOne[:, 0:(dataFileOne.shape[1] - countDelSpect)]\n",
    "        # 替换并删除尾部被剔除的光谱，不然后续均值方差图会出现很宽的方差bar\n",
    "        # **********************************************************************************************将信噪比保存到CSV\n",
    "\n",
    "        snrCSV = np.array(snrFileOne).reshape(-1, 1)\n",
    "        labelCSV = np.array(idLabelOne).reshape(-1, 1)\n",
    "        SNRClass = np.column_stack((snrCSV, labelCSV))\n",
    "        outPuthExcSNR = outPath + '/' + r'SNR(After processing).csv'\n",
    "        snrList = pd.DataFrame(SNRClass, columns=[\"SNR\", \"LabelName\"])\n",
    "        snrList.to_csv(outPuthExcSNR)\n",
    "        # *************************************************************************************************保存处理后数据\n",
    "        np.savetxt(outPath + '/' + files[0] + r'_Processed.txt',\n",
    "                   dataFileOne, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "        meanSpectrumOne.append(np.mean(dataFileOne[:, 1:dataFileOne.shape[1]], axis=1))\n",
    "        # 按照行进行均值拼接\n",
    "        stdSpectrumOne.append(np.std(dataFileOne[:, 1:dataFileOne.shape[1]], axis=1))\n",
    "        # 按照行进行标准差拼接\n",
    "\n",
    "        plt.plot(x, np.array(meanSpectrumOne).T, color=\"#1C1C1C\",\n",
    "                 lw=1,\n",
    "                 label='Spectral mean')\n",
    "\n",
    "        # plt.plot(x, np.array(meanSpectrum).T, color=\"white\", lw=2)\n",
    "        # print(np.array(x).shape)\n",
    "        std1 = (np.array(meanSpectrumOne) - np.array(stdSpectrumOne)).T\n",
    "        std2 = (np.array(meanSpectrumOne) + np.array(stdSpectrumOne)).T\n",
    "        # 方差的上下bar\n",
    "        plt.fill_between(x, std1.flatten(), std2.flatten(), color=\"#B5B5B5\", label='Std of SNR')\n",
    "        plt.legend(loc='best')\n",
    "        # flatten()二维转一维\n",
    "        outPathOne1 = outPath + '/' + files[0] + r'_meanStdOne.tif'\n",
    "        plt.savefig(outPathOne1, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathOne1 = outPath + '/' + files[0] + r'_meanStdOne.pdf'\n",
    "        plt.savefig(outPathOne1, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # **************************************************************************************************************tSNE\n",
    "        figure(2)\n",
    "        dataFileOne = dataFileOne[:, 1:dataFileOne.shape[1]].T\n",
    "\n",
    "        # print(dataFileOne.shape)\n",
    "        # return 0\n",
    "        # 去掉首列X信息，并转置\n",
    "        # 根据两个最大的主成分进行绘图\n",
    "\n",
    "        # nDim = 2\n",
    "        # #int, 空间的维度\n",
    "        # nPerplex = 10.0\n",
    "        # # 数据集越大，需要参数值越大，建议值位5 - 50\n",
    "        # learnRate = 200\n",
    "        # #float,学习率，建议取值为10.0-1000.0\n",
    "        # nInter = 1000\n",
    "        # # int, 最大迭代次数\n",
    "        # nGrad = 1e-06\n",
    "        # # float,如果梯度低于该值，则停止算法\n",
    "        resultOne = TSNE(n_components=nDim, perplexity=nPerplex, early_exaggeration=12.0, learning_rate=learnRate,\n",
    "                         n_iter=nInter,\n",
    "                         n_iter_without_progress=300, min_grad_norm=nGrad, metric='euclidean', init='pca',\n",
    "                         verbose=0, random_state=1, method='exact').fit_transform(dataFileOne)\n",
    "        # random_state:int, RandomState or None, default:None，伪随机数生成器种子,不同的初始化可能会导致成本函数的局部最小值不同\n",
    "        # print(\"***\",np.array(resultOne).shape)\n",
    "        # print(\"###\",np.array(dataFileOne).shape)\n",
    "\n",
    "        # # **************************************************************************************************************3 hca层次聚类\n",
    "        #\n",
    "        ySpec2 = dataFileOne\n",
    "\n",
    "        if distanceMethod == 0:\n",
    "            distM = \"ward\"\n",
    "            # 沃德方差最小化算法\n",
    "        elif distanceMethod == 1:\n",
    "            distM = \"centroid\"\n",
    "            # 质心间的欧式距离\n",
    "        elif distanceMethod == 2:\n",
    "            distM = \"weighted\"\n",
    "            # 加权分组平均法\n",
    "        else:\n",
    "            distM = 'average'\n",
    "            # UPGMA算法（非加权组平均）法\n",
    "\n",
    "        # distanceMethod = 'ward'\n",
    "        # # 层次聚类距离度量选用的方法,可选：'centroid','weighted','average'\n",
    "        Z = hierarchy.linkage(ySpec2, method=distM, metric='euclidean')\n",
    "        # 执行层次/聚集聚类\n",
    "\n",
    "        hcaColorThreshold = 10\n",
    "\n",
    "        # color_threshold= 8 设置颜色高度阈值，用于控制决策树和PCA的颜色分类\n",
    "        # hcaLabelsize = 6\n",
    "        # #标号大小\n",
    "        # cluster.hierarchy.cut_tree(Z, n_clusters=filesNumInter)\n",
    "        # 设置聚类个数n_clusters\n",
    "\n",
    "        hierarchy.dendrogram(Z, color_threshold=hcaColorThreshold, labels=pd.DataFrame(ySpec2).index + 1)\n",
    "        # 将分层聚类绘制为树状图\n",
    "        # hierarchy.dendrogram(Z, labels=pd.DataFrame(ySpec).index + 1)\n",
    "        # label = cluster.hierarchy.cut_tree(Z, height = hcaColorThreshold)\n",
    "        hacLeaves = hierarchy.leaves_list(Z) + 1\n",
    "        # 返回叶子节点\n",
    "        plt.tick_params(labelsize=hcaLabelsize)\n",
    "        # 调整刻度尺寸\n",
    "        plt.title('HCA')\n",
    "        # plt.show()\n",
    "        outPath7 = outPath + '/' + r'_HCAcluster.tif'\n",
    "        plt.savefig(outPath7, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath7 = outPath + '/' + r'_HCAcluster.pdf'\n",
    "        plt.savefig(outPath7, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        # # *******************************************************************************************保存层次聚类叶子节点\n",
    "        f = open(outPath + '/' + r'_HCAclusterLeaves.txt', \"w\")\n",
    "        f.write(\"层次聚类树状图叶子节点（从左到右排列）：\")\n",
    "        f.write('\\n')\n",
    "        f.write(str(hacLeaves))\n",
    "        f.close()\n",
    "\n",
    "        # ***************************************************************************************************HCA聚类评估\n",
    "        f = open(outPath + '/' + r'_HCA-evaluation(聚类评估算法).txt', \"w\", encoding='utf-8')\n",
    "        f.write(\"1.轮廓系数法：\\n\")\n",
    "        f.write(\"  轮廓系数（Silhouette Coefficient），是聚类效果好坏的一种评价方式,轮廓系数计算方式s=(b-a)/max(a,b)，\"\n",
    "                \"a：样本与同一簇中所有其他点之间的平均距离，b：样本与下一个最近的簇中所有其他点之间的平均距离。\"\n",
    "                \"最佳值为1，最差值为-1。接近0的值表示重叠的群集。负值通常表示样本已分配给错误的聚类，因为不同的聚类更为相似。\"\n",
    "                \"-1表示存在错误聚类，0表示有交叉的簇，1表示高密度聚类\\n\")\n",
    "        f.write(\"2.Calinski-Harabaz（CH）：\\n\")\n",
    "        f.write(\"  Calinski-Harabaz（CH）,CH指数S被定义为簇间离散与簇内离散的比率，\"\n",
    "                \"当簇内数据的协方差越小，簇间的协方差越大，Calinski-Harabasz分数越高，该分值越大说明聚类效果越好。\")\n",
    "        f.close()\n",
    "\n",
    "        # *************************************************************************************************轮廓系数K初值\n",
    "        K = 10\n",
    "        if dataFileOne.shape[1] < K:\n",
    "            clustersK = range(2, dataFileOne.shape[1])\n",
    "        else:\n",
    "            clustersK = range(2, K)\n",
    "        # clustersK = range(2, 10)\n",
    "        sc_scores = []\n",
    "        # 轮廓系数值\n",
    "        CHindexAgglomerativeClustering = []\n",
    "        # Calinski-Harabasz Index\n",
    "        for t in clustersK:\n",
    "            kmeans_model = AgglomerativeClustering(n_clusters=t, affinity='euclidean', memory=None, connectivity=None,\n",
    "                                                   compute_full_tree='auto', linkage='single').fit(dataFileOne)\n",
    "            sc_score = silhouette_score(dataFileOne, kmeans_model.labels_, metric='euclidean')\n",
    "            sc_scores.append(sc_score)\n",
    "            # SC系数(Silhouette Cofficient)轮廓系数法\n",
    "            '''\n",
    "            轮廓系数计算方式是，a：样本与同一类别中所有其他点之间的平均距离，b：样本与下一个最近的簇中所有其他点之间的平均距离。\n",
    "            单个样本的Silhouette系数s为：(b-a)/max(a,b),此函数返回所有样本的平均轮廓系数\n",
    "            仅当标签数为2 <= n_labels <= n_samples-1时，才定义轮廓系数。\n",
    "            最佳值为1，最差值为-1。接近0的值表示重叠的群集。负值通常表示样本已分配给错误的聚类，因为不同的聚类更为相​​似。\n",
    "            -1表示存在错误聚类，0表示有交叉的簇，1表示高密度聚类。\n",
    "            '''\n",
    "            CHindexAgglomerativeClustering.append(\n",
    "                metrics.calinski_harabasz_score(dataFileOne, kmeans_model.labels_))\n",
    "                #metrics.calinski_harabasz_score(X, y_pred)\n",
    "            # metrics.calinski_harabasz_score\n",
    "            # metrics.calinski_harabaz_score（注意区分两个包的区别）\n",
    "            '''\n",
    "            Calinski-Harabaz的分数S被定义为簇间离散与簇内离散的比率，该分值越大说明聚类效果越好。\n",
    "            '''\n",
    "        # kmeans_model = KMeans(n_clusters=3).fit(resultOne)\n",
    "        # sc_score = silhouette_score(resultOne, kmeans_model.labels_, metric='euclidean')\n",
    "        # print(\"##\",sc_score)\n",
    "        # return 0\n",
    "        fig, ax1 = plt.subplots(1, 1)  # 做1*1个子图，等价于 \" fig, ax1 = plt.subplot() \"\n",
    "        ax2 = ax1.twinx()  # 让2个子图的x轴一样，同时创建副坐标轴。\n",
    "        ax1.set_xlabel('Numbers of clusters(HCA)')\n",
    "        color1 = '#228B22'\n",
    "        ax1.set_ylabel('silhouette coefficient score', color=color1)\n",
    "        ax1.plot(clustersK, sc_scores, marker='o', color=color1)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "        color2 = '#8B6914'\n",
    "        ax2.set_ylabel('Calinski-Harabasz', color=color2)\n",
    "        ax2.plot(clustersK, CHindexAgglomerativeClustering, color=color2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        outPathHCA1 = outPath + '/' + r'_HCA-evaluation.tif'\n",
    "        plt.savefig(outPathHCA1, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathHCA2 = outPath + '/' + r'_HCA-evaluation.pdf'\n",
    "        plt.savefig(outPathHCA2, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ****************************************************************************************AgglomerativeClustering\n",
    "\n",
    "        # clusters_N = 2\n",
    "        clf = AgglomerativeClustering(n_clusters=clusters_N, affinity='euclidean', memory=None, connectivity=None,\n",
    "                                      compute_full_tree='auto', linkage='single')\n",
    "        # s = clf.fit(resultOne)\n",
    "        try:\n",
    "            s = clf.fit(dataFileOne)\n",
    "        except:\n",
    "            s = clf.fit(dataFileOne)\n",
    "        #\n",
    "        # clf = SpectralClustering(affinity='nearest_neighbors', n_clusters=clusters_N, n_neighbors=11).fit_predict(resultOne)\n",
    "        #\n",
    "        # print('KMeans参数为：', s)\n",
    "\n",
    "        # pcaSize = 30\n",
    "        # #pca标签大小\n",
    "        # pcaAlpha = 1\n",
    "        # #pca透明度\n",
    "        for i in range(resultOne.shape[0]):\n",
    "            plt.scatter(resultOne[i, pca_col1], resultOne[i, pca_col2],\n",
    "                        c=colorSpectrumColour[clf.labels_[i] % len(colorSpectrumColour)],\n",
    "                        marker=markers[clf.labels_[i] % len(markers)], s=pcaSize, alpha=pcaAlpha, edgecolor='k')\n",
    "\n",
    "        # # 画出聚类中心\n",
    "        # centerS = clf.cluster_centers_\n",
    "        # for i in range(clusters_N):\n",
    "        #     plt.scatter(centerS[i][0], centerS[i][1], c=\"red\", s=pcaSize + 6)\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，重新构建各值list\n",
    "        labelOneResult = []\n",
    "        # 返回标签给程序\n",
    "        labelOneResultL = []\n",
    "        # 返回标签给小翟\n",
    "        clusterSnr = []\n",
    "        # 存储聚类后的信噪比二维list\n",
    "        clusterStd = []\n",
    "        # 存储聚类后光谱标准差二维list\n",
    "        clusterData = []\n",
    "        # 存储聚类后光谱数据二维list\n",
    "        hcaDistance = []\n",
    "        # 存储聚类距离，保存聚类中心到各点距离\n",
    "        spectralDistance = []\n",
    "        # 存储谱聚类距离，保存聚类中心到各点距离\n",
    "        centerS_HCA = []\n",
    "        # 保存聚类中心，其实是计算kmeansDistance的均值（平面欧式距离）\n",
    "\n",
    "        # 创建二维列表\n",
    "        for i in range(clusters_N):\n",
    "            labelOneResult.append([])\n",
    "            labelOneResultL.append([])\n",
    "            clusterSnr.append([])\n",
    "            # clusterStd.append([])\n",
    "            clusterData.append([])\n",
    "            hcaDistance.append([])\n",
    "\n",
    "        # 根据聚类结果重新组织矩阵，每个子list存放一类\n",
    "        for i in range(clusters_N):\n",
    "            for j in range(len(clf.labels_)):\n",
    "                if (clf.labels_[j] == i):\n",
    "                    labelOneResult[i].append(clf.labels_[j])\n",
    "                    labelOneResultL[i].append(idLabelOne[j])\n",
    "                    clusterSnr[i].append(snrFileOne[j])\n",
    "                    clusterData[i].append(dataFileOne[j].T)\n",
    "                    hcaDistance[i].append([resultOne[j, 0], resultOne[j, 1]])\n",
    "                    # hca和谱聚类与kmeans不同，多一个计算过程\n",
    "            centerS_HCA.append(np.array(hcaDistance[i]).mean(axis=0).tolist())\n",
    "            # print(np.array(kmeansDistance[0]).mean(axis = 0)),列均值，求聚类散点的均值（均值即聚类中心，欧式距离）\n",
    "\n",
    "        # # 画出聚类中心\n",
    "        # centerS = clf.cluster_centers_\n",
    "        centerS_HCA = np.array(centerS_HCA)\n",
    "        for i in range(clusters_N):\n",
    "            plt.scatter(centerS_HCA[i][0], centerS_HCA[i][1], c=\"red\", s=pcaSize + 6)\n",
    "\n",
    "        # plt.legend(loc=0, ncol=1)\n",
    "        plt.xlabel(u\"tSNE 1\")\n",
    "        plt.ylabel(u\"tSNE 2\")\n",
    "        plt.title('HCA-tSNE')\n",
    "        # plt.show()\n",
    "        outPath9 = outPath + '/' + r'_HCA-tSNE.tif'\n",
    "        plt.savefig(outPath9, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath9 = outPath + '/' + r'_HCA-tSNE.pdf'\n",
    "        plt.savefig(outPath9, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ***************************************************************************************************************计算聚类中心到聚类数据距离\n",
    "        # 存储每类均谱\n",
    "        clusterMeanS = []\n",
    "        clusterStdS = []\n",
    "        std1 = []\n",
    "        std2 = []\n",
    "        f = open(outPath + '/' + r'_HCA-tSNE(Clustering evaluation).txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            clusterMeanS.append(np.mean(clusterData[i], axis=0))\n",
    "            clusterStdS.append(np.std(clusterData[i], axis=0))\n",
    "            std1.append((np.array(clusterMeanS[i]) - np.array(clusterStdS[i])))\n",
    "            std2.append((np.array(clusterMeanS[i]) + np.array(clusterStdS[i])))\n",
    "            # print(\"kmeans1:\",np.mean(cdist([centerS_HCA[i].tolist()],kmeansDistance[i] , metric='euclidean')))\n",
    "            # print(\"center1:\",\n",
    "            #       (np.sum(cdist([centerS_HCA[i].tolist()], centerS_HCA, metric='euclidean'))) / (len(centerS_HCA) - 1))\n",
    "            f.write(\"HCA聚类结果中，第\" + str(i + 1) + \"堆聚类中心到该堆内所有点欧式距离均值：\" + str(\"%.2f\" % (np.mean(\n",
    "                cdist([centerS_HCA[i].tolist()], hcaDistance[i], metric='euclidean')))) + \"\\n\")\n",
    "\n",
    "            f.write(\"HCA聚类结果中，第\" + str(i + 1) + \"堆聚类中心到其他所有聚类中心欧式距离均值：\" + str(\n",
    "                \"%.2f\" % ((np.sum(cdist([centerS_HCA[i].tolist()], centerS_HCA, metric='euclidean'))) / (\n",
    "                        len(centerS_HCA) - 1))) + \"\\n\")\n",
    "            # (len(centerS)-1):聚类中心点个数-1，当前点到其他聚类中心的均值\n",
    "            # cdist([centerS[i].tolist()], centerS, metric='euclidean')，点到其他所有点距离\n",
    "            # np.sum：距离求和，均值需要除以n-1，即：len(centerS)-1\n",
    "        # f.write(\"\\n\\n标签顺序：(1)□(2)◇(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·(13)正Y(14)◁(15)▷(16)△(也可按箱线图或均值方差罗列图颜色对应)\")\n",
    "        f.write(\n",
    "            \"\\n\\n标签顺序：(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·\"\n",
    "            \"(13)正Y(14)左三角形(15)右三角形(16)◇(也可按箱线图或均值方差罗列图颜色对应)\")\n",
    "        f.close()\n",
    "\n",
    "        # **************************************************************************************************************保存HCA聚类均值\n",
    "        hcaclusterMeanS = np.column_stack((x, np.array(clusterMeanS).T))\n",
    "        np.savetxt(outPath + '/' + r'_HCA-means.txt',\n",
    "                   hcaclusterMeanS, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，画均值方差图\n",
    "        figure(figsize=(8, 12))\n",
    "        for interI in range(clusters_N):\n",
    "            interIhight = 1\n",
    "            # print(\"1:\",np.array(x).shape)\n",
    "            # print(\"2:\",np.array(clusterData[interI]).shape)\n",
    "            # return 0\n",
    "            plt.plot(x, (clusterMeanS[interI] + interIhight * interI),\n",
    "                     color=colorSpectrumColour[interI % len(colorSpectrumColour)], lw=1)\n",
    "            plt.fill_between(x, (std1[interI]) + interIhight * interI, (std2[interI]) + interIhight * interI,\n",
    "                             color=\"#B5B5B5\")\n",
    "        plt.title('MeanStd column chart', fontsize=12)\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.legend(loc = 'best')\n",
    "        outPath4 = outPath + '/' + r'_HCA-tSNEmeanStd.tif'\n",
    "        plt.savefig(outPath4, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath4 = outPath + '/' + r'_HCA-tSNEmeanStd.pdf'\n",
    "        plt.savefig(outPath4, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #\n",
    "        # for i in range(clusters_N):\n",
    "        #     meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #     # 按照行进行均值拼接\n",
    "        #     stdSpectrum.append(np.std(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，画箱线图\n",
    "        # figure()\n",
    "        dfLists = []\n",
    "        # 存储以文件为单位信噪比字典\n",
    "        for interI in range(clusters_N):\n",
    "            dfLists.append(pd.DataFrame({\"cluster:\" + str(interI + 1): clusterSnr[interI]}))\n",
    "        dfData = pd.concat(dfLists, axis=1)\n",
    "\n",
    "        f = dfData.boxplot(sym='+', patch_artist=True, return_type='dict')\n",
    "\n",
    "        for box, c in zip(f['boxes'], colorSpectrumColour):\n",
    "            # 箱体边框颜色\n",
    "            box.set(color=c, linewidth=2)\n",
    "            # 箱体内部填充颜色\n",
    "            box.set(facecolor=c)\n",
    "\n",
    "        plt.grid(axis=\"y\", ls=\":\", lw=1, color=\"gray\", alpha=0.8)\n",
    "\n",
    "        outPath41 = outPath + '/' + r'_HCA-tSNEbox.tif'\n",
    "        plt.savefig(outPath41, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath41 = outPath + '/' + r'_HCA-tSNEbox.pdf'\n",
    "        plt.savefig(outPath41, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ***************************************************************************************************************保存txt聚类结果\n",
    "\n",
    "        f = open(outPath + '/' + r'_HCA-tSNE.txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            f.write(str(i + 1) + \"\\t\")\n",
    "            for j in range(len(labelOneResultL[i])):\n",
    "                # f.write(str((labelOneResultL[i])[j]) + \",\")\n",
    "                f.write(str((labelOneResultL[i])[j]))\n",
    "                if j < len(labelOneResultL[i]) - 1:\n",
    "                    f.write(\",\")\n",
    "            f.write('\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        # ************************************************************************************************************************************kemans聚类\n",
    "        # ****************************************************************************************基于轮廓系数的kmeans聚类评估\n",
    "        sc_scores = []\n",
    "        # 轮廓系数值\n",
    "        CHindexKmeans = []\n",
    "        # Calinski-Harabasz Index\n",
    "        for t in clustersK:\n",
    "            kmeans_model = KMeans(n_clusters=t).fit(dataFileOne)\n",
    "            sc_score = silhouette_score(dataFileOne, kmeans_model.labels_, metric='euclidean')\n",
    "            sc_scores.append(sc_score)\n",
    "            # SC系数(Silhouette Cofficient)轮廓系数法\n",
    "            '''\n",
    "            轮廓系数计算方式是，a：样本与同一类别中所有其他点之间的平均距离，b：样本与下一个最近的簇中所有其他点之间的平均距离。\n",
    "            单个样本的Silhouette系数s为：(b-a)/max(a,b),此函数返回所有样本的平均轮廓系数\n",
    "            仅当标签数为2 <= n_labels <= n_samples-1时，才定义轮廓系数。\n",
    "            最佳值为1，最差值为-1。接近0的值表示重叠的群集。负值通常表示样本已分配给错误的聚类，因为不同的聚类更为相​​似。\n",
    "            -1表示存在错误聚类，0表示有交叉的簇，1表示高密度聚类。\n",
    "            '''\n",
    "            labelsKm = metrics.calinski_harabasz_score(dataFileOne, kmeans_model.labels_)\n",
    "            CHindexKmeans.append(labelsKm)\n",
    "\n",
    "            '''\n",
    "            Calinski-Harabaz的分数S被定义为簇间离散与簇内离散的比率，该分值越大说明聚类效果越好。\n",
    "            '''\n",
    "        # kmeans_model = KMeans(n_clusters=3).fit(resultOne)\n",
    "        # sc_score = silhouette_score(resultOne, kmeans_model.labels_, metric='euclidean')\n",
    "        # print(\"##\",sc_score)\n",
    "        # return 0\n",
    "        fig, ax1 = plt.subplots(1, 1)  # 做1*1个子图，等价于 \" fig, ax1 = plt.subplot() \"\n",
    "        ax2 = ax1.twinx()  # 让2个子图的x轴一样，同时创建副坐标轴。\n",
    "        ax1.set_xlabel('Numbers of clusters(Kmeans)')\n",
    "        color1 = '#228B22'\n",
    "        ax1.set_ylabel('silhouette coefficient score', color=color1)\n",
    "        ax1.plot(clustersK, sc_scores, marker='o', color=color1)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "        color2 = '#8B6914'\n",
    "        ax2.set_ylabel('Calinski-Harabasz', color=color2)\n",
    "        ax2.plot(clustersK, CHindexKmeans, color=color2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        outPathKm1 = outPath + '/' + r'_kMeans-evaluation.tif'\n",
    "        plt.savefig(outPathKm1, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathKm2 = outPath + '/' + r'_kMeans-evaluation.pdf'\n",
    "        plt.savefig(outPathKm2, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # *****************************************************************************************************kemans聚类\n",
    "        # clusters_N = 2\n",
    "        clf = KMeans(n_clusters=clusters_N)\n",
    "        # clf = AgglomerativeClustering(n_clusters=clusters_N, affinity='euclidean', memory=None, connectivity=None,\n",
    "        #                               compute_full_tree='auto', linkage='single')\n",
    "        # s = clf.fit(resultOne)\n",
    "        try:\n",
    "            s = clf.fit(dataFileOne)\n",
    "        except:\n",
    "            s = clf.fit(dataFileOne)\n",
    "        # pcaSize = 30\n",
    "        # #pca标签大小\n",
    "        # pcaAlpha = 1\n",
    "        # #pca透明度\n",
    "        for i in range(resultOne.shape[0]):\n",
    "            plt.scatter(resultOne[i, pca_col1], resultOne[i, pca_col2],\n",
    "                        c=colorSpectrumColour[clf.labels_[i] % len(colorSpectrumColour)],\n",
    "                        marker=markers[clf.labels_[i] % len(markers)], s=pcaSize, alpha=pcaAlpha, edgecolor='k')\n",
    "\n",
    "        # 画出聚类中心\n",
    "        centerS = clf.cluster_centers_\n",
    "        # print(\"centerS:\",centerS.shape)\n",
    "        # print(\"centerS:\",centerS[0])\n",
    "        # return 0\n",
    "        for i in range(clusters_N):\n",
    "            plt.scatter(centerS[i][0], centerS[i][1], c=\"red\", s=pcaSize + 6)\n",
    "\n",
    "        # for tsneI in range(3):\n",
    "        #     tsneCount2 = tsneCount2 + pcaCount[tsneI]\n",
    "        #     plt.scatter(result2[tsneCount1:tsneCount2, pca_col1], result2[tsneCount1:tsneCount2, pca_col2],\n",
    "        #                 c=colorSpectrumColour[tsneI % len(colorSpectrumColour)], marker=markers[tsneI % len(markers)],\n",
    "        #                 s=pcaSize,\n",
    "        #                 alpha=pcaAlpha, edgecolor='k', label=files[tsneI])\n",
    "        #     tsneCount1 = tsneCount1 + pcaCount[tsneI]\n",
    "\n",
    "        # plt.legend(loc=0, ncol=1)\n",
    "        plt.xlabel(u\"tSNE 1\")\n",
    "        plt.ylabel(u\"tSNE 2\")\n",
    "        plt.title('kMeans-tSNE')\n",
    "        # plt.show()\n",
    "        outPath9 = outPath + '/' + r'_kMeans-TSNE.tif'\n",
    "        plt.savefig(outPath9, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath9 = outPath + '/' + r'_kMeans-TSNE.pdf'\n",
    "        plt.savefig(outPath9, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # **************************************************************************************************************根据聚类结果，重新构建各值list\n",
    "        labelOneResult = []\n",
    "        # 返回标签给程序\n",
    "        labelOneResultL = []\n",
    "        # 返回标签给小翟\n",
    "        clusterSnr = []\n",
    "        # 存储聚类后的信噪比二维list\n",
    "        clusterStd = []\n",
    "        # 存储聚类后光谱标准差二维list\n",
    "        clusterData = []\n",
    "        # 存储聚类后光谱数据二维list\n",
    "        kmeansDistance = []\n",
    "        # 存储聚类距离，保存聚类中心到各点距离\n",
    "\n",
    "        # 创建二维列表\n",
    "        for i in range(clusters_N):\n",
    "            labelOneResult.append([])\n",
    "            labelOneResultL.append([])\n",
    "            clusterSnr.append([])\n",
    "            # clusterStd.append([])\n",
    "            clusterData.append([])\n",
    "            kmeansDistance.append([])\n",
    "\n",
    "        # 根据聚类结果重新组织矩阵，每个子list存放一类\n",
    "        for i in range(clusters_N):\n",
    "            for j in range(len(clf.labels_)):\n",
    "                if (clf.labels_[j] == i):\n",
    "                    labelOneResult[i].append(clf.labels_[j])\n",
    "                    labelOneResultL[i].append(idLabelOne[j])\n",
    "                    clusterSnr[i].append(snrFileOne[j])\n",
    "                    clusterData[i].append(dataFileOne[j].T)\n",
    "                    kmeansDistance[i].append([resultOne[j, 0], resultOne[j, 1]])\n",
    "\n",
    "        # ***************************************************************************************************************计算聚类中心到聚类数据距离\n",
    "        # 存储每类均谱\n",
    "        clusterMeanS = []\n",
    "        clusterStdS = []\n",
    "        std1 = []\n",
    "        std2 = []\n",
    "        f = open(outPath + '/' + r'_kMeans-tSNE(Clustering evaluation).txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            clusterMeanS.append(np.mean(clusterData[i], axis=0))\n",
    "            clusterStdS.append(np.std(clusterData[i], axis=0))\n",
    "            std1.append((np.array(clusterMeanS[i]) - np.array(clusterStdS[i])))\n",
    "            std2.append((np.array(clusterMeanS[i]) + np.array(clusterStdS[i])))\n",
    "            # print(\"kmeans1:\",np.mean(cdist([centerS_HCA[i].tolist()],kmeansDistance[i] , metric='euclidean')))\n",
    "            # print(\"center1:\",\n",
    "            #       (np.sum(cdist([centerS_HCA[i].tolist()], centerS_HCA, metric='euclidean'))) / (len(centerS_HCA) - 1))\n",
    "\n",
    "            # f.write(\"Kmeans聚类结果中，第\" + str(i + 1) + \"堆聚类中心到该堆内所有点欧式距离均值：\" + str(\n",
    "            #     \"%.2f\" % (np.mean(cdist([centerS[i].tolist()], kmeansDistance[i], metric='euclidean')))) + \"\\n\")\n",
    "            # f.write(\"Kmeans聚类结果中，第\" + str(i + 1) + \"堆聚类中心到其他所有聚类中心欧式距离均值：\" + str(\n",
    "            #     \"%.2f\" % ((np.sum(cdist([centerS[i].tolist()], centerS, metric='euclidean'))) / (\n",
    "            #             len(centerS) - 1))) + \"\\n\")\n",
    "            # (len(centerS)-1):聚类中心点个数-1，当前点到其他聚类中心的均值\n",
    "            # cdist([centerS[i].tolist()], centerS, metric='euclidean')，点到其他所有点距离\n",
    "            # np.sum：距离求和，均值需要除以n-1，即：len(centerS)-1\n",
    "        # f.write(\"\\n\\n标签顺序：(1)□(2)◇(3)○(4)＋(5)▽(6)♢(7)×(8)☆(9)“正六边形”(10)“左Y”(11)“正五边形”\n",
    "        # (12)·(13)“正Y”(14)◁(15)▷(16)△(也可按箱线图或均值方差罗列图颜色对应)\")\n",
    "        f.write(\n",
    "            \"\\n\\n标签顺序：(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·\"\n",
    "            \"(13)正Y(14)左三角形(15)右三角形(16)◇(也可按箱线图或均值方差罗列图颜色对应)\")\n",
    "        f.close()\n",
    "        # **************************************************************************************************************保存谱kmeans聚类均值\n",
    "        hcaclusterMeanS = np.column_stack((x, np.array(clusterMeanS).T))\n",
    "        np.savetxt(outPath + '/' + r'_kMeans-means.txt',\n",
    "                   hcaclusterMeanS, delimiter='\\t', fmt='%.2f')\n",
    "        # **************************************************************************************************************根据聚类结果，画均值方差图\n",
    "        figure(figsize=(8, 12))\n",
    "        for interI in range(clusters_N):\n",
    "            interIhight = 0.6\n",
    "            # print(\"1:\",np.array(x).shape)\n",
    "            # print(\"2:\",np.array(clusterData[interI]).shape)\n",
    "            # return 0\n",
    "            plt.plot(x, (clusterMeanS[interI] + interIhight * interI),\n",
    "                     color=colorSpectrumColour[interI % len(colorSpectrumColour)], lw=1)\n",
    "            plt.fill_between(x, (std1[interI]) + interIhight * interI, (std2[interI]) + interIhight * interI,\n",
    "                             color=\"#B5B5B5\")\n",
    "        plt.title('MeanStd column chart', fontsize=12)\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.legend(loc = 'best')\n",
    "        outPath4 = outPath + '/' + r'_kMeans-tSNEmeanStd.tif'\n",
    "        plt.savefig(outPath4, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath4 = outPath + '/' + r'_kMeans-tSNEmeanStd.pdf'\n",
    "        plt.savefig(outPath4, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #\n",
    "        # for i in range(clusters_N):\n",
    "        #     meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #     # 按照行进行均值拼接\n",
    "        #     stdSpectrum.append(np.std(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，画箱线图\n",
    "        # figure()\n",
    "        dfLists = []\n",
    "        # 存储以文件为单位信噪比字典\n",
    "        for interI in range(clusters_N):\n",
    "            dfLists.append(pd.DataFrame({\"cluster:\" + str(interI + 1): clusterSnr[interI]}))\n",
    "        dfData = pd.concat(dfLists, axis=1)\n",
    "\n",
    "        f = dfData.boxplot(sym='+', patch_artist=True, return_type='dict')\n",
    "\n",
    "        for box, c in zip(f['boxes'], colorSpectrumColour):\n",
    "            # 箱体边框颜色\n",
    "            box.set(color=c, linewidth=2)\n",
    "            # 箱体内部填充颜色\n",
    "            box.set(facecolor=c)\n",
    "\n",
    "        plt.grid(axis=\"y\", ls=\":\", lw=1, color=\"gray\", alpha=0.8)\n",
    "\n",
    "        outPath41 = outPath + '/' + r'_kMeans-tSNEbox.tif'\n",
    "        plt.savefig(outPath41, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath41 = outPath + '/' + r'_kMeans-tSNEbox.pdf'\n",
    "        plt.savefig(outPath41, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ***************************************************************************************************************保存txt聚类结果\n",
    "\n",
    "        f = open(outPath + '/' + r'_kMeans-tSNE.txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            f.write(str(i + 1) + \"\\t\")\n",
    "            for j in range(len(labelOneResultL[i])):\n",
    "                # f.write(str((labelOneResultL[i])[j]) + \",\")\n",
    "                f.write(str((labelOneResultL[i])[j]))\n",
    "                if j < len(labelOneResultL[i]) - 1:\n",
    "                    f.write(\",\")\n",
    "            f.write('\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        # return 0\n",
    "\n",
    "        # # *************************************************************************************************************再次聚类，最大堆聚类\n",
    "        # temp = []\n",
    "        # for i in range(clusters_N):\n",
    "        #     temp.append(np.array(clusterData[i]).shape[0])\n",
    "        # temp = np.array(temp)\n",
    "        # # temp = np.array([np.array(clusterData[0]).shape[0],np.array(clusterData[1]).shape[0],\n",
    "        # #         np.array(clusterData[2]).shape[0],np.array(clusterData[3]).shape[0],\n",
    "        # #         np.array(clusterData[4]).shape[0]])\n",
    "        #\n",
    "        # max_index = np.argmax(temp)\n",
    "        #\n",
    "        # # print(\"00:\", labelOneResultL[max_index])\n",
    "        #\n",
    "        # labelOneResultLMax = labelOneResultL[max_index]\n",
    "        # # 存放最大堆标签\n",
    "        #\n",
    "        # resultOne1 = TSNE(n_components=nDim, perplexity=nPerplex, early_exaggeration=12.0, learning_rate=learnRate,\n",
    "        #                   n_iter=nInter,\n",
    "        #                   n_iter_without_progress=300, min_grad_norm=nGrad, metric='euclidean', init='pca',\n",
    "        #                   verbose=0, random_state=None, method='exact').fit_transform(clusterData[max_index])\n",
    "        #\n",
    "        # clusters_N1 = 3\n",
    "        #\n",
    "        # clf1 = KMeans(n_clusters=clusters_N1, max_iter=300, tol=1e-3, init='random', n_init=20,\n",
    "        #               precompute_distances='auto', algorithm='auto')\n",
    "        # clf1.fit(resultOne1)\n",
    "        #\n",
    "        # for i in range(resultOne1.shape[0]):\n",
    "        #     plt.scatter(resultOne1[i, pca_col1], resultOne1[i, pca_col2],\n",
    "        #                 c=colorSpectrumColour[clf1.labels_[i] % len(colorSpectrumColour)],\n",
    "        #                 marker=markers[clf1.labels_[i] % len(markers)], s=pcaSize, alpha=pcaAlpha, edgecolor='k')\n",
    "        #\n",
    "        # # 画出聚类中心\n",
    "        # centerS = clf1.cluster_centers_\n",
    "        # for i in range(clusters_N1):\n",
    "        #     plt.scatter(centerS[i][0], centerS[i][1], c=\"red\", s=pcaSize + 6)\n",
    "        # # plt.scatter(centerS[1][0], centerS[1][1], c=\"red\", s=pcaSize + 6)\n",
    "        # # plt.scatter(centerS[2][0], centerS[2][1], c=\"red\", s=pcaSize + 6)\n",
    "        #\n",
    "        # # plt.legend(loc=0, ncol=1)\n",
    "        # plt.xlabel(u\"tSNE 1\")\n",
    "        # plt.ylabel(u\"tSNE 2\")\n",
    "        # plt.title('kMeans-tSNE')\n",
    "        # # plt.show()\n",
    "        # outPath5 = outPath + '\\\\' + r'_kMeans-TSNE(max).tif'\n",
    "        # plt.savefig(outPath5, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # plt.close()\n",
    "        # # **************************************************************************************************************再次聚类结果，重新构建各值list\n",
    "        # labelOneResult2 = []\n",
    "        # # 返回标签给程序\n",
    "        # labelOneResultL2 = []\n",
    "        # # 返回标签给小翟\n",
    "        # clusterSnr2 = []\n",
    "        # clusterData2 = []\n",
    "        # # 创建二维列表\n",
    "        # for i in range(clusters_N1):\n",
    "        #     labelOneResult2.append([])\n",
    "        #     labelOneResultL2.append([])\n",
    "        #     clusterSnr2.append([])\n",
    "        #     # clusterStd2.append([])\n",
    "        #     clusterData2.append([])\n",
    "        # # 根据聚类结果重新组织矩阵，每个子list存放一类\n",
    "        # for i in range(clusters_N1):\n",
    "        #     for j in range(len(clf1.labels_)):\n",
    "        #         if (clf1.labels_[j] == i):\n",
    "        #             labelOneResult2[i].append(clf1.labels_[j])\n",
    "        #             labelOneResultL2[i].append(labelOneResultLMax[j])\n",
    "        #             clusterSnr2[i].append(snrFileOne[j])\n",
    "        #             clusterData2[i].append(dataFileOne[j].T)\n",
    "        #\n",
    "        # # 存储每类均谱\n",
    "        # clusterMeanS2 = []\n",
    "        # clusterStdS2 = []\n",
    "        # Std1 = []\n",
    "        # Std2 = []\n",
    "        # for i in range(clusters_N1):\n",
    "        #     clusterMeanS2.append(np.mean(clusterData2[i], axis=0))\n",
    "        #     clusterStdS2.append(np.std(clusterData2[i], axis=0))\n",
    "        #     Std1.append((np.array(clusterMeanS2[i]) - np.array(clusterStdS2[i])))\n",
    "        #     Std2.append((np.array(clusterMeanS2[i]) + np.array(clusterStdS2[i])))\n",
    "        #\n",
    "        # # **************************************************************************************************************根据max聚类结果，再次画均值方差图\n",
    "        #\n",
    "        # for interI in range(clusters_N1):\n",
    "        #     interIhight = 0.5\n",
    "        #     # print(\"1:\",np.array(x).shape)\n",
    "        #     # print(\"2:\",np.array(clusterData[interI]).shape)\n",
    "        #     # return 0\n",
    "        #     plt.plot(x, (clusterMeanS2[interI] + interIhight * interI),\n",
    "        #              color=colorSpectrumColour[interI % len(colorSpectrumColour)], lw=1)\n",
    "        #     plt.fill_between(x, (Std1[interI]) + interIhight * interI, (Std2[interI]) + interIhight * interI,\n",
    "        #                      color=\"#B5B5B5\")\n",
    "        # plt.title('MeanStd column chart', fontsize=14)\n",
    "        # # plt.show()\n",
    "        #\n",
    "        # # plt.legend(loc = 'best')\n",
    "        # outPath5 = outPath + '\\\\' + r'_kMeans-TSNE(max)meanStd.tif'\n",
    "        # plt.savefig(outPath5, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # # plt.show()\n",
    "        # plt.close()\n",
    "        #\n",
    "        # # # **************************************************************************************************************根据max聚类结果，再次画箱线图\n",
    "        # # # figure()\n",
    "        # dfLists2 = []\n",
    "        # # 存储以文件为单位信噪比字典\n",
    "        # for interI in range(clusters_N1):\n",
    "        #     dfLists2.append(pd.DataFrame({\"cluster(max):\" + str(interI + 1): clusterSnr2[interI]}))\n",
    "        # dfData2 = pd.concat(dfLists2, axis=1)\n",
    "        #\n",
    "        # f2 = dfData2.boxplot(sym='+', patch_artist=True, return_type='dict')\n",
    "        #\n",
    "        # for box, c in zip(f2['boxes'], colorSpectrumColour):\n",
    "        #     # 箱体边框颜色\n",
    "        #     box.set(color=c, linewidth=2)\n",
    "        #     # 箱体内部填充颜色\n",
    "        #     box.set(facecolor=c)\n",
    "        #\n",
    "        # plt.grid(axis=\"y\", ls=\":\", lw=1, color=\"gray\", alpha=0.8)\n",
    "        #\n",
    "        # outPath51 = outPath + '\\\\' + r'_kMeans-TSNE(max)box.tif'\n",
    "        # plt.savefig(outPath51, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # # plt.show()\n",
    "        # plt.close()\n",
    "        #\n",
    "        # # ***************************************************************************************************************再次max聚类保存txt结果\n",
    "        #\n",
    "        # f = open(outPath + '\\\\' + r'_kMeans-TSNE(max).txt', \"w\")\n",
    "        # for i in range(clusters_N1):\n",
    "        #     f.write(str(i + 1) + \"\\t\")\n",
    "        #     for j in range(len(labelOneResultL2[i])):\n",
    "        #         # f.write(str((labelOneResultL[i])[j]) + \",\")\n",
    "        #         f.write(str((labelOneResultL2[i])[j]))\n",
    "        #         if j < len(labelOneResultL2[i]) - 1:\n",
    "        #             f.write(\",\")\n",
    "        #     f.write('\\n')\n",
    "        #\n",
    "        # f.close()\n",
    "\n",
    "        # ************************************************************************************************************************************谱聚类\n",
    "        # ****************************************************************************************基于轮廓系数的谱聚类评估\n",
    "        # clustersK = range(2, 10)\n",
    "        sc_scores = []\n",
    "        # 轮廓系数值\n",
    "        CHindexSpectralClustering = []\n",
    "        # Calinski-Harabasz Index\n",
    "        for t in clustersK:\n",
    "            SpectralClustering_model = SpectralClustering(affinity='nearest_neighbors', n_clusters=t,\n",
    "                                                          n_neighbors=11).fit(resultOne)\n",
    "            sc_score = silhouette_score(resultOne, SpectralClustering_model.labels_, metric='euclidean')\n",
    "            sc_scores.append(sc_score)\n",
    "            # SC系数(Silhouette Cofficient)轮廓系数法\n",
    "            '''\n",
    "            轮廓系数计算方式是，a：样本与同一类别中所有其他点之间的平均距离，b：样本与下一个最近的簇中所有其他点之间的平均距离。\n",
    "            单个样本的Silhouette系数s为：(b-a)/max(a,b),此函数返回所有样本的平均轮廓系数\n",
    "            仅当标签数为2 <= n_labels <= n_samples-1时，才定义轮廓系数。\n",
    "            最佳值为1，最差值为-1。接近0的值表示重叠的群集。负值通常表示样本已分配给错误的聚类，因为不同的聚类更为相​​似。\n",
    "            -1表示存在错误聚类，0表示有交叉的簇，1表示高密度聚类。            \n",
    "            '''\n",
    "            CHindexSpectralClustering.append(\n",
    "                metrics.calinski_harabasz_score(resultOne, SpectralClustering_model.labels_))\n",
    "\n",
    "            '''\n",
    "            Calinski-Harabaz的分数S被定义为簇间离散与簇内离散的比率，该分值越大说明聚类效果越好。\n",
    "            '''\n",
    "        # kmeans_model = KMeans(n_clusters=3).fit(resultOne)\n",
    "        # sc_score = silhouette_score(resultOne, kmeans_model.labels_, metric='euclidean')\n",
    "        # print(\"##\",sc_score)\n",
    "        # return 0\n",
    "        fig, ax1 = plt.subplots(1, 1)  # 做1*1个子图，等价于 \" fig, ax1 = plt.subplot() \"\n",
    "        ax2 = ax1.twinx()  # 让2个子图的x轴一样，同时创建副坐标轴。\n",
    "        ax1.set_xlabel('Numbers of clusters(SpectralClustering)')\n",
    "        color1 = '#228B22'\n",
    "        ax1.set_ylabel('silhouette coefficient score', color=color1)\n",
    "        ax1.plot(clustersK, sc_scores, marker='o', color=color1)\n",
    "        ax1.tick_params(axis='y', labelcolor=color1)\n",
    "\n",
    "        color2 = '#8B6914'\n",
    "        ax2.set_ylabel('Calinski-Harabasz', color=color2)\n",
    "        ax2.plot(clustersK, CHindexSpectralClustering, color=color2)\n",
    "        ax2.tick_params(axis='y', labelcolor=color2)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        outPathSC2 = outPath + '/' + r'_SClustering-evaluation.tif'\n",
    "        plt.savefig(outPathSC2, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathSC21 = outPath + '/' + r'_SClustering-evaluation.pdf'\n",
    "        plt.savefig(outPathSC21, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # *********************************************************************************************************谱聚类\n",
    "        clf = SpectralClustering(affinity='nearest_neighbors', n_clusters=clusters_N, n_neighbors=11)\n",
    "        try:\n",
    "            s = clf.fit_predict(resultOne)\n",
    "        except:\n",
    "            s = clf.fit_predict(resultOne)\n",
    "        # s = clf.fit_predict(resultOne)\n",
    "        for i in range(resultOne.shape[0]):\n",
    "            plt.scatter(resultOne[i, pca_col1], resultOne[i, pca_col2],\n",
    "                        c=colorSpectrumColour[clf.labels_[i] % len(colorSpectrumColour)],\n",
    "                        marker=markers[clf.labels_[i] % len(markers)], s=pcaSize, alpha=pcaAlpha, edgecolor='k')\n",
    "\n",
    "        # for tsneI in range(3):\n",
    "        #     tsneCount2 = tsneCount2 + pcaCount[tsneI]\n",
    "        #     plt.scatter(result2[tsneCount1:tsneCount2, pca_col1], result2[tsneCount1:tsneCount2, pca_col2],\n",
    "        #                 c=colorSpectrumColour[tsneI % len(colorSpectrumColour)], marker=markers[tsneI % len(markers)],\n",
    "        #                 s=pcaSize,\n",
    "        #                 alpha=pcaAlpha, edgecolor='k', label=files[tsneI])\n",
    "        #     tsneCount1 = tsneCount1 + pcaCount[tsneI]\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，重新构建各值list\n",
    "        labelOneResult = []\n",
    "        # 返回标签给程序\n",
    "        labelOneResultL = []\n",
    "        # 返回标签给小翟\n",
    "        clusterSnr = []\n",
    "        # 存储聚类后的信噪比二维list\n",
    "        clusterStd = []\n",
    "        # 存储聚类后光谱标准差二维list\n",
    "        clusterData = []\n",
    "        # 存储聚类后光谱数据二维list\n",
    "        spectralDistance = []\n",
    "        # 存储谱聚类距离，保存聚类中心到各点距离\n",
    "        centerS_SC = []\n",
    "        # 保存聚类中心，其实是计算kmeansDistance的均值（平面欧式距离）\n",
    "\n",
    "        # 创建二维列表\n",
    "        for i in range(clusters_N):\n",
    "            labelOneResult.append([])\n",
    "            labelOneResultL.append([])\n",
    "            clusterSnr.append([])\n",
    "            # clusterStd.append([])\n",
    "            clusterData.append([])\n",
    "            spectralDistance.append([])\n",
    "        # 根据聚类结果重新组织矩阵，每个子list存放一类\n",
    "        for i in range(clusters_N):\n",
    "            for j in range(len(clf.labels_)):\n",
    "                if (clf.labels_[j] == i):\n",
    "                    labelOneResult[i].append(clf.labels_[j])\n",
    "                    labelOneResultL[i].append(idLabelOne[j])\n",
    "                    clusterSnr[i].append(snrFileOne[j])\n",
    "                    clusterData[i].append(dataFileOne[j].T)\n",
    "                    spectralDistance[i].append([resultOne[j, 0], resultOne[j, 1]])\n",
    "            centerS_SC.append(np.array(spectralDistance[i]).mean(axis=0).tolist())\n",
    "            # print(np.array(kmeansDistance[0]).mean(axis = 0)),列均值，求聚类散点的均值（均值即聚类中心，欧式距离）\n",
    "        # 存储每类均谱\n",
    "        centerS_SC = np.array(centerS_SC)\n",
    "        for i in range(clusters_N):\n",
    "            plt.scatter(centerS_SC[i][0], centerS_SC[i][1], c=\"red\", s=pcaSize + 6)\n",
    "\n",
    "        # plt.legend(loc=0, ncol=1)\n",
    "        plt.xlabel(u\"tSNE 1\")\n",
    "        plt.ylabel(u\"tSNE 2\")\n",
    "        plt.title('SpectralClustering-tSNE')\n",
    "        # plt.show()\n",
    "        outPathSC1 = outPath + '/' + r'_SClustering-tSNE.tif'\n",
    "        plt.savefig(outPathSC1, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathSC1 = outPath + '/' + r'_SClustering-tSNE.pdf'\n",
    "        plt.savefig(outPathSC1, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ***************************************************************************************************************计算聚类中心到聚类数据距离\n",
    "\n",
    "        clusterMeanS = []\n",
    "        clusterStdS = []\n",
    "        std1 = []\n",
    "        std2 = []\n",
    "        f = open(outPath + '/' + r'_SClustering-tSNE(Clustering evaluation).txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            clusterMeanS.append(np.mean(clusterData[i], axis=0))\n",
    "            clusterStdS.append(np.std(clusterData[i], axis=0))\n",
    "            std1.append((np.array(clusterMeanS[i]) - np.array(clusterStdS[i])))\n",
    "            std2.append((np.array(clusterMeanS[i]) + np.array(clusterStdS[i])))\n",
    "\n",
    "            f.write(\"SClustering聚类结果中，第\" + str(i + 1) + \"堆聚类中心到该堆内所有点欧式距离均值：\" + str(\n",
    "                \"%.2f\" % (np.mean(cdist([centerS_SC[i].tolist()], spectralDistance[i], metric='euclidean')))) + \"\\n\")\n",
    "\n",
    "            f.write(\"SClustering聚类结果中，第\" + str(i + 1) + \"堆聚类中心到其他所有聚类中心欧式距离均值：\" + str(\n",
    "                \"%.2f\" % ((np.sum(cdist([centerS_SC[i].tolist()], centerS_SC, metric='euclidean'))) / (\n",
    "                        len(centerS_SC) - 1))) + \"\\n\")\n",
    "            # (len(centerS)-1):聚类中心点个数-1，当前点到其他聚类中心的均值\n",
    "            # cdist([centerS[i].tolist()], centerS, metric='euclidean')，点到其他所有点距离\n",
    "            # np.sum：距离求和，均值需要除以n-1，即：len(centerS)-1\n",
    "\n",
    "        f.write(\n",
    "            \"\\n\\n标签顺序：(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·\"\n",
    "            \"(13)正Y(14)左三角形(15)右三角形(16)◇(也可按箱线图或均值方差罗列图颜色对应)\")\n",
    "        f.close()\n",
    "        # print(\"kmeans3:\", np.mean(cdist([centerS_SC[i].tolist()], kmeansDistance[i], metric='euclidean')))\n",
    "        #\n",
    "        # for i in range(0,clusters_N):\n",
    "        #     print(\"center3:\",(np.sum(cdist([centerS_SC[i].tolist()], centerS_SC, metric='euclidean')))/(len(centerS_SC)-1))\n",
    "        # **************************************************************************************************************保存谱聚类聚类均值\n",
    "        hcaclusterMeanS = np.column_stack((x, np.array(clusterMeanS).T))\n",
    "        np.savetxt(outPath + '/' + r'_SClustering-means.txt',\n",
    "                   hcaclusterMeanS, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，画均值方差图\n",
    "        figure(figsize=(8, 12))\n",
    "        for interI in range(clusters_N):\n",
    "            interIhight = 0.6\n",
    "            # print(\"1:\",np.array(x).shape)\n",
    "            # print(\"2:\",np.array(clusterData[interI]).shape)\n",
    "            # return 0\n",
    "            plt.plot(x, (clusterMeanS[interI] + interIhight * interI),\n",
    "                     color=colorSpectrumColour[interI % len(colorSpectrumColour)], lw=1)\n",
    "            plt.fill_between(x, (std1[interI]) + interIhight * interI, (std2[interI]) + interIhight * interI,\n",
    "                             color=\"#B5B5B5\")\n",
    "        plt.title('MeanStd column chart_SC', fontsize=12)\n",
    "        # plt.show()\n",
    "\n",
    "        # plt.legend(loc = 'best')\n",
    "        outPathSC2 = outPath + '/' + r'_SClustering-tSNEmeanStd.tif'\n",
    "        plt.savefig(outPathSC2, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathSC2 = outPath + '/' + r'_SClustering-tSNEmeanStd.pdf'\n",
    "        plt.savefig(outPathSC2, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #\n",
    "        # for i in range(clusters_N):\n",
    "        #     meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "        #     # 按照行进行均值拼接\n",
    "        #     stdSpectrum.append(np.std(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "\n",
    "        # **************************************************************************************************************根据聚类结果，画箱线图\n",
    "        # figure()\n",
    "        dfLists = []\n",
    "        # 存储以文件为单位信噪比字典\n",
    "        for interI in range(clusters_N):\n",
    "            dfLists.append(pd.DataFrame({\"cluster:\" + str(interI + 1): clusterSnr[interI]}))\n",
    "        dfData = pd.concat(dfLists, axis=1)\n",
    "\n",
    "        f = dfData.boxplot(sym='+', patch_artist=True, return_type='dict')\n",
    "\n",
    "        for box, c in zip(f['boxes'], colorSpectrumColour):\n",
    "            # 箱体边框颜色\n",
    "            box.set(color=c, linewidth=2)\n",
    "            # 箱体内部填充颜色\n",
    "            box.set(facecolor=c)\n",
    "\n",
    "        plt.grid(axis=\"y\", ls=\":\", lw=1, color=\"gray\", alpha=0.8)\n",
    "\n",
    "        outPathSC3 = outPath + '/' + r'_SClustering-tSNEbox.tif'\n",
    "        plt.savefig(outPathSC3, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPathSC3 = outPath + '/' + r'_SClustering-tSNEbox.pdf'\n",
    "        plt.savefig(outPathSC3, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ***************************************************************************************************************保存txt聚类结果\n",
    "\n",
    "        f = open(outPath + '/' + r'_SClustering-tSNE.txt', \"w\")\n",
    "        for i in range(clusters_N):\n",
    "            f.write(str(i + 1) + \"\\t\")\n",
    "            for j in range(len(labelOneResultL[i])):\n",
    "                # f.write(str((labelOneResultL[i])[j]) + \",\")\n",
    "                f.write(str((labelOneResultL[i])[j]))\n",
    "                if j < len(labelOneResultL[i]) - 1:\n",
    "                    f.write(\",\")\n",
    "            f.write('\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        # ***************************************************************************************************************word\n",
    "        # print(\"删除标签：\", idLabelOneDel)\n",
    "        # print(\"标签：\", idLabelOne)\n",
    "        # print(\"聚类：\", clf.labels_)\n",
    "        # print(\"11\",labelOneResult)\n",
    "        # print(\"22\", labelOneResultL)\n",
    "        document = Document()\n",
    "        # 打开文档\n",
    "        document.styles['Normal'].font.name = u'宋体'\n",
    "        document.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'宋体')\n",
    "        # 设置文档格式\n",
    "        # t0：标题\n",
    "        t0 = document.add_heading(u'实验报告', 1)\n",
    "        t0.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "        # 直接添加标题，“1”表示1级标题,标题居中\n",
    "        # t1,t2,t3次级标题\n",
    "        t1 = document.add_heading(u'1.数据信息', 2)\n",
    "        # 增加表格1\n",
    "        rows_num = 3\n",
    "        cols_num = 2\n",
    "        table1 = document.add_table(rows=rows_num, cols=cols_num, style='Table Grid')\n",
    "        table1.style.font.name = u'宋体'\n",
    "        listTable1 = ['选中光谱ID', '剔除光谱ID', '保留光谱ID']\n",
    "        # 列标签\n",
    "        for i in range(rows_num):\n",
    "            table1.cell(i, 0).text = listTable1[i]\n",
    "        # 行信息\n",
    "        table1.cell(0, 1).text = str(idLabel)\n",
    "        # table1.cell(1, 1).text = str(snrDelfileOne)\n",
    "        table1.cell(1, 1).text = str(idLabelOneDel)\n",
    "        table1.cell(2, 1).text = str(idLabelOne)\n",
    "\n",
    "        t2 = document.add_heading(u'2.参数信息', 2)\n",
    "        # 增加表格2\n",
    "        listTable2 = [\"参数名\", \"参数意义\", \"参数值\"]\n",
    "        listTable2Name = [\"dataPath\", \"outPath\", \"startSpec\", \"endSpec\", \"snrNoiseindex\", \"delSNRmin\", \"delSNRmax\",\n",
    "                          \"filterSize\",\n",
    "                          \"dynamicFactor\", \"winSG\", \"nSG\", \"lambdaAirPls\", \"itermaxAirPls\", \"pltInter\",\n",
    "                          \"processLinewidth\",\n",
    "                          \"distanceMethod\", \"hcaLabelsize\", \"pcaNcomponents\", \"pca_col1\", \"pca_col2\", \"pcaSize\",\n",
    "                          \"pcaAlpha\",\n",
    "                          \"nDim\", \"nPerplex\", \"learnRate\", \"nInter\", \"nGrad\", \"svmTestSize\", \"svmC\", \"xKernel\",\n",
    "                          \"clusters_N\"]\n",
    "        listTable2Meaning = [\"输入路径\", \"输出路径\", \"截谱长度(首)\", \"截谱长度(尾)\", \"静默区：尾端长度\", \"信噪比最小阈值\", \"信噪比最大阈值\",\n",
    "                             \"宇宙射线：窗口宽度\", \"宇宙射线：动态因子\", \"滤波：窗口宽度\", \"滤波：拟合阶次\",\n",
    "                             \"基线校正：lambda\", \"基线校正：迭代次数\", \"显示光谱量(未使用)\", \"光谱线宽(未使用)\",\n",
    "                             \"HCA距离度量方法:\\r0:沃德方差最小化\\r1:质心欧式距离\\r2:加权分组平均\\r3:UPGMA算法\",\n",
    "                             \"HCA(tSNE)标号大小\", \"PCA主成分个数\", \"PCA(tSNE)选择维度\", \"PCA(tSNE)选择维度\",\n",
    "                             \"PCA(tSNE)标签大小\", \"PCA(tSNE)透明度\", \"tSNE空间维度\", \"Perplex\", \"tSNE学习率\",\n",
    "                             \"tSNE最大迭代次数\", \"tSNE收敛误差\", \"测试数据占比\", \"SVM错误项惩罚系数\",\n",
    "                             \"SVM核函数:\\r0:多项式核\\r1:线性核\\r2:径向基核\\r3:sigmod核\\r4:核矩阵\", \"聚类堆数\"]\n",
    "        listTable2Value = [dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize,\n",
    "                           dynamicFactor, winSG,\n",
    "                           nSG, lambdaAirPls, itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize,\n",
    "                           pcaNcomponents, pca_col1, pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter,\n",
    "                           nGrad, svmTestSize, svmC, xKernel, clusters_N]\n",
    "\n",
    "        rows_num = len(listTable2Meaning) + 1\n",
    "        cols_num = len(listTable2)\n",
    "        table2 = document.add_table(rows=rows_num, cols=cols_num, style='Table Grid')\n",
    "        table2.style.font.name = u'宋体'\n",
    "        # 列标题\n",
    "        for j in range(0, cols_num):\n",
    "            table2.cell(0, j).text = listTable2[j]\n",
    "        # 行标题\n",
    "        for i in range(1, rows_num):\n",
    "            table2.cell(i, 0).text = listTable2Name[i - 1]\n",
    "            table2.cell(i, 1).text = listTable2Meaning[i - 1]\n",
    "            table2.cell(i, 2).text = str(listTable2Value[i - 1])\n",
    "        # 均值方差图\n",
    "        t3 = document.add_heading(u'3.全部数据均值方差图', 2)\n",
    "        table31 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table31.style.font.name = u'宋体'\n",
    "        table31.cell(0, 0).text = \"注：全部光谱数据的均值和标准差\"\n",
    "        # table12.cell(0, 1).text = str(\"%.2f%%\" % (scoreLDA * 100))\n",
    "        img_name = outPath + '/' + files[0] + r'_meanStdOne.tif'\n",
    "        fig2 = document.add_picture(img_name, width=Inches(5.0))\n",
    "        # 向文档里添加图片\n",
    "        # fig2.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "        # #居中显示\n",
    "        t7 = document.add_heading(u'4.HCA树状图', 2)\n",
    "        table71 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table71.style.font.name = u'宋体'\n",
    "        table71.cell(0, 0).text = \"注：层次聚类树状图叶子节点（从左到右排列）,详见_hcaclusterLeaves.txt文件。\"\n",
    "        img_name = outPath + '/' + r'_HCAcluster.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # tSNE\n",
    "        t8 = document.add_heading(u'5.tSNE空间分布图', 2)\n",
    "        table81 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table81.style.font.name = u'宋体'\n",
    "        # table81.cell(0, 0).text = \"注：散点标签见文件夹下_HCA-tSNE.txt文件；\\n\\t标签顺序(也可按颜色对应)：(1)□(2)◇(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·(13)正Y(14)左三角形(15)右三角形(16)△\"\n",
    "        table81.cell(0,\n",
    "                     0).text = \"注：散点标签见文件夹下_HCA-tSNE.txt文件；\\n\\t标签顺序(也可按颜色对应)：\" \\\n",
    "                               \"(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·\" \\\n",
    "                               \"(13)正Y(14)左三角形(15)右三角形(16)◇\"\n",
    "\n",
    "        img_name = outPath + '/' + r'_HCA-tSNE.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # 层次聚类均值方差图\n",
    "        t9 = document.add_heading(u'6.层次聚类均值方差罗列图', 2)\n",
    "        table91 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table91.style.font.name = u'宋体'\n",
    "        table91.cell(0, 0).text = \"注：光谱数据从下往上排列，按颜色对应[_HCA-tSNE.txt]光谱聚类点分布\"\n",
    "        img_name = outPath + '/' + r'_HCA-tSNEmeanStd.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # 层次聚类箱线图\n",
    "        t10 = document.add_heading(u'7.层次聚类信噪比箱线图', 2)\n",
    "        table101 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table101.style.font.name = u'宋体'\n",
    "        table101.cell(0, 0).text = \"注：从左往右按颜色对应方差罗列排列\"\n",
    "        img_name = outPath + '/' + r'_HCA-tSNEbox.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # kmeans均值\n",
    "        t11 = document.add_heading(u'8.kMeans聚类图', 2)\n",
    "        table111 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table111.style.font.name = u'宋体'\n",
    "        table111.cell(0,\n",
    "                      0).text = \"注：散点标签见文件夹下[_kMeans-tSNE.txt]文件；\\n\\t标签顺序(也可按颜色对应)：\" \\\n",
    "                                \"(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形(12)·\" \\\n",
    "                                \"(13)正Y(14)左三角形(15)右三角形(16)◇\\n\\t红色圆点为聚类中心点(平面欧式距离中心)\"\n",
    "        img_name = outPath + '/' + r'_kMeans-TSNE.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # kemans聚类均值方差罗列图\n",
    "        t92 = document.add_heading(u'9.kMeans聚类均值方差罗列图', 2)\n",
    "        table912 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table912.style.font.name = u'宋体'\n",
    "        table912.cell(0, 0).text = \"注：光谱数据从下往上排列，按颜色对应[_kMeans-tSNE.txt]光谱聚类点分布\"\n",
    "        img_name = outPath + '/' + r'_kMeans-tSNEmeanStd.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # kmeans聚类箱线图\n",
    "        t102 = document.add_heading(u'10.kMeans聚类信噪比箱线图', 2)\n",
    "        table102 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table102.style.font.name = u'宋体'\n",
    "        table102.cell(0, 0).text = \"注：从左往右按颜色对应方差罗列排列\"\n",
    "        img_name = outPath + '/' + r'_kMeans-tSNEbox.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # tSNE-spectralCluster\n",
    "        tSC = document.add_heading(u'11.SpectralCluster-tSNE空间分布图', 2)\n",
    "        tableSC1 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        tableSC1.style.font.name = u'宋体'\n",
    "        tableSC1.cell(0, 0).text = \"注：散点标签见文件夹下_SClustering-tSNE.txt文件；\\n\\t标签顺序(也可按颜色对应)：\" \\\n",
    "                                   \"(1)□(2)△(3)○(4)＋(5)▽(6)长菱形(7)×(8)☆(9)正六边形(10)左Y(11)正五边形\" \\\n",
    "                                   \"(12)·(13)正Y(14)左三角形(15)右三角形(16)◇\"\n",
    "        img_name = outPath + '/' + r'_SClustering-tSNE.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # 谱聚类均值方差图\n",
    "        tSCstd = document.add_heading(u'12.谱聚类均值方差罗列图', 2)\n",
    "        tableSCstd1 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        tableSCstd1.style.font.name = u'宋体'\n",
    "        tableSCstd1.cell(0, 0).text = \"注：光谱数据从下往上排列，按颜色对应[_SClustering-tSNE.txt]光谱聚类点分布\"\n",
    "        img_name = outPath + '/' + r'_SClustering-tSNEmeanStd.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        # 谱聚类箱线图\n",
    "        t10 = document.add_heading(u'13.谱聚类信噪比箱线图', 2)\n",
    "        table101 = document.add_table(rows=1, cols=1, style='Table Grid')\n",
    "        table101.style.font.name = u'宋体'\n",
    "        table101.cell(0, 0).text = \"注：从左往右按颜色对应方差罗列排列\"\n",
    "        img_name = outPath + '/' + r'_SClustering-tSNEbox.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        document.save(outPath + '/' + r\"HookeReport_Onefile.docx\")\n",
    "        # 保存文档\n",
    "        return 1\n",
    "    # *******************************************************************************************************************************多个文件夹\n",
    "    else:\n",
    "        # 1读取文夹并合并所有文件\n",
    "        dirList = []\n",
    "        # 作为变量名\n",
    "        filesDataName = copy1.copy(files)\n",
    "        # 作为同名变量数据\n",
    "        pcaCountAll = []\n",
    "        # 用于存放每个标签数量\n",
    "        pcaCount = []\n",
    "        # 用于存储删除光谱后标签数量\n",
    "        # i = 0\n",
    "        filesNumInter = 0\n",
    "        # filesNumInter 全局文档个数，迭代时“从0开始，最后加1”\n",
    "        classLabel = []\n",
    "        # 创建标签\n",
    "        classLabelName = []\n",
    "        # 和标签对应的名字\n",
    "        classOneLabel = []\n",
    "        # 创建单个标签\n",
    "        snrAllfile = []\n",
    "        snrAllfile1 = []\n",
    "        # 存储所有文件光谱信噪比\n",
    "        allData = []\n",
    "        # 用于存储所有处理后的数据\n",
    "        # print(len(classOneLabel))\n",
    "        meanAllSpectrum = []\n",
    "        # 用于存储所有文件光谱均值，画均值方差层次罗列图\n",
    "        stdAllSpectrum = []\n",
    "        # 用于保存标准差到txt\n",
    "        stdAll1 = []\n",
    "        stdAll2 = []\n",
    "        # 分别用于存储均值方差图上下bar，阴影部分\n",
    "        snrDelfile = []\n",
    "        # 用于存储删除信噪比小于阈值的光谱\n",
    "        for f in files:\n",
    "            filesDataName[filesNumInter] = []\n",
    "            pltY = []\n",
    "            # 存储光谱y值，用于有层次画图\n",
    "            meanSpectrum = []\n",
    "            # 存储该文件下光谱均值\n",
    "            stdSpectrum = []\n",
    "            # 存储该文件夹下光谱标准差\n",
    "            if os.path.isdir(dataPath + '/' + f):  # 判断是不是文件夹\n",
    "                # if f[0] != '.':  # 排除隐藏文件夹。因为隐藏文件夹过多\n",
    "                dirList.append(f)  # 添加非隐藏文件夹\n",
    "\n",
    "            filesDataName[filesNumInter] = readMerge(dataPath[:] + \"/\" + dirList[filesNumInter])\n",
    "            # print(\"***\",np.array(filesDataName).shape)\n",
    "            # return 0\n",
    "            filesDataNameTemp = filesDataName[filesNumInter]\n",
    "            # allData = readMerge(dataPath[:] + \"\\\\\" + dirList[i])\n",
    "            pcaCountAll.append(filesDataNameTemp.shape[1] - 1)\n",
    "            # print('pcaCount:',pcaCount)\n",
    "            # 把该标签数据量添加到pcaCount中\n",
    "\n",
    "            filesDataNameTemp = filesDataNameTemp[startSpec: (filesDataNameTemp.shape[0] - endSpec), :]\n",
    "            # #截谱\n",
    "            # # **************************************************************************************************前后截谱\n",
    "            # if filesNumInter == 0:\n",
    "            #     filesDataNameTemp = filesDataNameTemp[100:, :]\n",
    "            # elif filesNumInter == 1:\n",
    "            #     filesDataNameTemp = filesDataNameTemp\n",
    "            # elif filesNumInter == 2:\n",
    "            #     filesDataNameTemp = filesDataNameTemp[100:, :]\n",
    "            # else:\n",
    "            #     filesDataNameTemp = filesDataNameTemp\n",
    "\n",
    "            filesDataName1 = copy1.copy(filesDataNameTemp)\n",
    "            # 注意：直接采用 ’ = ’ 赋值方式会使得两个变量占用同一地址\n",
    "            x = filesDataNameTemp[:, 0]\n",
    "\n",
    "            snrOnefile = []\n",
    "            # 存储单个文件下拉曼光谱信噪比\n",
    "            snrOneDelfile = []\n",
    "            # 用于存储单个文件下删除信噪比小于阈值的光谱\n",
    "\n",
    "            countDelSpect = 0\n",
    "            # 用于存储该文件夹下被删除文件的数量\n",
    "            # 2批处理\n",
    "            for j in range(1, filesDataNameTemp.shape[1]):\n",
    "                snrS = 0\n",
    "                # 初始化该光谱信噪比\n",
    "                # 循环处理每个光谱\n",
    "                y = filesDataNameTemp[:, j]\n",
    "                # 从第二列开始，保留了第一列坐标，后面需要去掉\n",
    "                # y1 = Norris(y, nNorris)\n",
    "\n",
    "                y1 = CRR(y, filterSize, dynamicFactor)\n",
    "                y2 = savgol_filter(y1, winSG, nSG)\n",
    "                # 2.3 airPLS\n",
    "                # print(\"j:\",j)\n",
    "                y3 = airPLS(y2, lambdaAirPls, itermaxAirPls)\n",
    "                # 有参去背景（自适应迭代重加权惩罚最小二乘，airpls）\n",
    "                # y3 = background_substruction(y2)\n",
    "                # 无参去背景（交替最小二乘ALS）\n",
    "                # 2.4 归一化\n",
    "                y4 = MaxMinNormalization(y3)\n",
    "                # snrNoiseindex = 20\n",
    "                # # 从尾部选取长度，作为静默区\n",
    "                # delSNR = 10\n",
    "                # #删除光谱信噪比阈值\n",
    "\n",
    "                # snrSpectrumSingal = max(y4[170 - startSpec:400 - startSpec]) - min(y4)\n",
    "                snrSpectrumSingal = max(y4) - min(y4)\n",
    "                # 信号强度\n",
    "                snrSpectrumNoise = max(y4[-snrNoiseindex:]) - min(y4[-snrNoiseindex:])\n",
    "                # 噪声强度\n",
    "                snrS = snrSpectrumSingal / snrSpectrumNoise\n",
    "                # 光谱信噪比\n",
    "                if snrS < delSNRmin or snrS > delSNRmax:\n",
    "                    snrOneDelfile.append(j)\n",
    "                    countDelSpect = countDelSpect + 1\n",
    "                    continue\n",
    "\n",
    "                snrOnefile.append(snrS)\n",
    "                snrAllfile1.append(snrS)\n",
    "                # 全部信噪比值存储到一个list中\n",
    "\n",
    "                filesDataNameTemp[:, j - countDelSpect] = y4\n",
    "                # j - countDelSpect,用于减去删除光谱的数量\n",
    "                # if j <= pltInter:\n",
    "                #     pltTemp = y4 + j * 0.3\n",
    "                #     pltY.append(pltTemp)\n",
    "                #     # 用于层次化画光谱\n",
    "                #     # np.column_stack((Data_x, Dout.T))\n",
    "                classLabel.append(filesNumInter)\n",
    "                classLabelName.append(files[filesNumInter])\n",
    "\n",
    "                # print(\"snrOnefile\",snrOnefile)\n",
    "            filesDataNameTemp = filesDataNameTemp[:, 0:(filesDataNameTemp.shape[1] - countDelSpect)]\n",
    "\n",
    "            # ************************************************************************************************************保存处理后数据\n",
    "            np.savetxt(outPath + '/' + r'Process_' + files[filesNumInter] + '.txt',\n",
    "                       filesDataNameTemp, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "            # print(\"删除文件数量：\",countDelSpect)\n",
    "            snrDelfile.append(snrOneDelfile)\n",
    "            # 用于存储所有信噪比小于阈值的光谱\n",
    "            pcaCount.append(filesDataNameTemp.shape[1] - 1)\n",
    "            # allData.append(filesDataNameTemp)\n",
    "            # 存储所有处理后的光谱数据\n",
    "            snrAllfile.append(snrOnefile)\n",
    "            # 按文件存储所有光谱信噪比\n",
    "            # print(\"filesDataNameTemp:\",np.array(filesDataNameTemp).shape)\n",
    "            # print(\"pltY\",np.array(pltY).shape)\n",
    "            classOneLabel.append(filesNumInter)\n",
    "            # 存储标签名，只包含1个\n",
    "            # urldic = urlparse(dataPath)\n",
    "            # pathdict = urldic.path.split('\\\\')\n",
    "            # # 以\"\\\"分割路径，供后文输出路径地址\n",
    "            # outPath1 = outPath + '\\\\' + files[i] + r'_process.txt'\n",
    "            # 输出地址及其文件名\n",
    "            filesDataName[filesNumInter] = filesDataNameTemp\n",
    "            # np.savetxt(outPath1, filesDataName[i], fmt='%f', delimiter=' ')\n",
    "\n",
    "            # **********************************************************************************************************批处理画图,处理前\n",
    "            # figure(3 * filesNumInter)\n",
    "            # # processLinewidth = 0.5\n",
    "            # # #线宽\n",
    "            # endPlt = min(pltInter, filesDataName1.shape[1])\n",
    "            # plt.plot(x, filesDataName1[:, 1:endPlt], linewidth=processLinewidth)\n",
    "            # # 显示网格\n",
    "            # plt.grid(linestyle='-.')\n",
    "            # plt.tight_layout()\n",
    "            # # plt.show()\n",
    "            # outPath1 = outPath + '\\\\' + files[filesNumInter] + r'_origin.tif'\n",
    "            # plt.savefig(outPath1, bbox_inches='tight')\n",
    "            # **********************************************************************************************************批处理画图处理后\n",
    "            # figure(3 * filesNumInter + 1)\n",
    "            # plt.plot(x, np.array(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]]), linewidth=processLinewidth)\n",
    "            # # 显示网格\n",
    "            # plt.grid(linestyle='-.')\n",
    "            # plt.tight_layout()\n",
    "            # # plt.show()\n",
    "            # outPath2 = outPath + '\\\\' + files[filesNumInter] + r'_process.tif'\n",
    "            # plt.savefig(outPath2, bbox_inches='tight')\n",
    "            # # print(\"111:\",np.array(filesDataNameTemp).shape)\n",
    "            # **********************************************************************************************************处理后均值方差图\n",
    "            figure(3 * filesNumInter + 2)\n",
    "            meanSpectrum.append(np.mean(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "            # 按照行进行均值拼接\n",
    "            stdSpectrum.append(np.std(filesDataNameTemp[:, 1:filesDataNameTemp.shape[1]], axis=1))\n",
    "\n",
    "            # # 查找标准差最大处的峰位\n",
    "            # print(np.array(stdSpectrum).shape)\n",
    "            # print(\"1:\",stdSpectrum[0].tolist())\n",
    "            # # print(x[stdSpectrum.index(max(stdSpectrum[0], key=abs))])\n",
    "            # print(x[stdSpectrum[0].tolist().index(max(stdSpectrum[0].tolist(), key=abs))])\n",
    "\n",
    "            # 按照行进行标准差拼接\n",
    "            stdAllSpectrum.append(stdSpectrum)\n",
    "            # 拼接标准差\n",
    "            plt.plot(x, np.array(meanSpectrum).T, color=colorSpectrumColour[filesNumInter % len(colorSpectrumColour)],\n",
    "                     lw=1,\n",
    "                     label='Spectral mean')\n",
    "            # plt.plot(x, np.array(meanSpectrum).T, color=\"white\", lw=2)\n",
    "            # print(np.array(x).shape)\n",
    "            # print(\"111:\",(np.array(meanSpectrum).T).shape)\n",
    "            # print(\"222:\",((np.array(meanSpectrum) - np.array(stdSpectrum)).T).shape)\n",
    "            std1 = (np.array(meanSpectrum) - np.array(stdSpectrum)).T\n",
    "            std2 = (np.array(meanSpectrum) + np.array(stdSpectrum)).T\n",
    "            plt.fill_between(x, std1.flatten(), std2.flatten(), color=\"#B5B5B5\", label='Std of SNR')\n",
    "            plt.legend(loc='best')\n",
    "            # flatten()二维转一维\n",
    "            outPath3 = outPath + '/' + files[filesNumInter] + r'_meanStd.tif'\n",
    "            plt.savefig(outPath3, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath3 = outPath + '/' + files[filesNumInter] + r'_meanStd.pdf'\n",
    "            plt.savefig(outPath3, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "\n",
    "            meanAllSpectrum.append(np.array(meanSpectrum).T)\n",
    "            # # 用于存储所有文件光谱均值，画均值方差层次罗列图\n",
    "            stdAll1.append(std1.flatten())\n",
    "            # a是个数组，a.flatten()就是把a降到一维，默认是按行的方向降\n",
    "            stdAll2.append(std2.flatten())\n",
    "            # **********************************************************************************************************保存均值\n",
    "            # print(\"???\",type(x))\n",
    "            # print(\"???\",type(y))\n",
    "            xMean = np.array(x).reshape(-1, 1)\n",
    "            yMeam = np.array(meanSpectrum).reshape(-1, 1)\n",
    "            # print(xMean.shape)\n",
    "            # print(yMeam.shape)\n",
    "            dataMean = np.hstack((xMean, yMeam))\n",
    "\n",
    "            np.savetxt(outPath + '/' + r'Mean_' + files[filesNumInter] + '.txt',\n",
    "                       dataMean, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "            # plt.show()\n",
    "\n",
    "            filesNumInter = filesNumInter + 1\n",
    "            plt.close('all')\n",
    "        # print(\"allData1:\",np.array(allData).shape)\n",
    "        # **********************************************************************************************将信噪比保存到CSV,信噪比箱型抖动图\n",
    "\n",
    "        snrCSV = np.array(snrAllfile1).reshape(-1, 1)\n",
    "        labelCSV = np.array(classLabelName).reshape(-1, 1)\n",
    "        SNRClass = np.column_stack((snrCSV, labelCSV))\n",
    "        outPuthExcSNR = outPath + '/' + r'SNR.csv'\n",
    "        snrList = pd.DataFrame(SNRClass, columns=[\"SNR\", \"LabelName\"])\n",
    "        snrList.to_csv(outPuthExcSNR)\n",
    "\n",
    "        dfBox = pd.read_csv(outPuthExcSNR)\n",
    "        sns.boxplot(x='LabelName', y='SNR', data=dfBox)\n",
    "        # sns.violinplot(x='SNR', y='LabelName', data=df)\n",
    "        # 小提琴形状替换箱线图\n",
    "        sns.stripplot(x='LabelName', y='SNR', data=dfBox, jitter=0.25, palette='Set2', dodge=True, size=4, alpha=0.5,\n",
    "                      color=None, )\n",
    "        # 可以再加一个分量hue，可用来控制每组抖动点进一步划分不同颜色！！！！！\n",
    "        # linewidth=1,加水平网格\n",
    "        # plt.legend()\n",
    "        plt.xticks(fontsize=12, rotation=90)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.xlabel('Labels', fontsize=12)  # 添加x轴的名称\n",
    "        plt.ylabel('SNR intensity', fontsize=12)\n",
    "        # plt.show()\n",
    "        # plt.ylim([1.5,15])\n",
    "        # # 设置坐标范围\n",
    "\n",
    "        outPath6 = outPath + '/' + r'_snrBox1.tif'\n",
    "        plt.savefig(outPath6, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath6 = outPath + '/' + r'_snrBox1.pdf'\n",
    "        plt.savefig(outPath6, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # **************************************************************************************************将均谱保存txt\n",
    "        meanAllSpectrumTuple0 = np.array(meanAllSpectrum).shape[0]\n",
    "        # tuple索引\n",
    "        meanAllSpectrumTuple1 = np.array(meanAllSpectrum).shape[1]\n",
    "        meanAllSpectrumTxt = np.array(meanAllSpectrum).reshape(meanAllSpectrumTuple0, meanAllSpectrumTuple1).T\n",
    "        # reshape变形，(m,n,1)到(m,n)\n",
    "        meanAllSpectrumTxt = np.column_stack((x.T, meanAllSpectrumTxt))\n",
    "        # 列拼接\n",
    "        np.savetxt(outPath + '/' + r'_spectrumMean.txt', meanAllSpectrumTxt, delimiter='\\t', fmt='%.2f')\n",
    "\n",
    "        # **************************************************************************************************************均值方差层次罗列图\n",
    "        figure(3 * filesNumInter, figsize=(8, 12))\n",
    "\n",
    "        # data = np.array(\n",
    "        #     pd.read_csv(r\"C:\\Users\\lixin\\Desktop\\11.txt\", delim_whitespace=True,\n",
    "        #                 header=None))\n",
    "        # x = data[50:,0]\n",
    "\n",
    "        for interI in range(filesNumInter):\n",
    "            interIhight = 0.6\n",
    "            plt.plot(x, meanAllSpectrum[interI] + interIhight * interI,\n",
    "                     color=colorSpectrumColour[interI % len(colorSpectrumColour)], lw=1, label=files[interI])\n",
    "            plt.fill_between(x, stdAll1[interI] + interIhight * interI, stdAll2[interI] + interIhight * interI,\n",
    "                             color=\"#B5B5B5\")\n",
    "            # plt.title('MeanStd column chart', fontsize=12)\n",
    "            plt.legend(bbox_to_anchor=(0, 1.02, 1, 0.2), loc=\"lower left\",\n",
    "                       mode=\"expand\", borderaxespad=0, ncol=3)\n",
    "        plt.yticks([])\n",
    "        # 不显示纵坐标\n",
    "        # plt.legend(loc = 2)\n",
    "        plt.rcParams.update({'font.size': 12})\n",
    "        # plt.show()\n",
    "        outPath4 = outPath + '/' + r'_meanStdcolumn.tif'\n",
    "        plt.savefig(outPath4, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath4 = outPath + '/' + r'_meanStdcolumn.pdf'\n",
    "        plt.savefig(outPath4, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ****************************************************************************************************************保存标准差信息\n",
    "        # stdAllSpectrum\n",
    "        f = open(outPath + '/' + r'_meanStdcolumn.txt', \"w\")\n",
    "        f.write('保存同类拉曼光谱所有拉曼位移的标准差' + '\\n')\n",
    "        for interI in range(filesNumInter):\n",
    "            # f.write('保存同类拉曼光谱所有拉曼位移的标准差：' + '\\n')\n",
    "            f.write(\"[\" + str(files[interI]) + ']光谱的平均标准差为：' + str(round(np.mean(stdAllSpectrum[interI]), 2)) + '\\n')\n",
    "            # f.write(\"[\" + str(files[interI]) + ']光谱的平均标准差为：' + str(round(np.mean(stdAll1[interI]), 2)) + '\\n')\n",
    "\n",
    "            f.write(\"[\" + str(files[interI]) + ']光谱的最大标准差为：' + str(\n",
    "                round(np.max(stdAllSpectrum[interI]), 2)) + \",最大处峰位：\" + str(\n",
    "                x[stdAllSpectrum[interI][0].tolist().index(max(stdAllSpectrum[interI][0].tolist(), key=abs))]) + '\\n')\n",
    "            # x[stdSpectrum[0].tolist().index(max(stdSpectrum[0].tolist(), key=abs))]\n",
    "\n",
    "            # f.write(\"[\" + str(files[interI]) + ']光谱的最大标准差为：' + str(\n",
    "            #     round(np.max(stdAll1[interI]), 2)) + \",最大处峰位：\" + str(\n",
    "            #     x[stdAll1[interI].index(max(stdAll1[interI][interI], key=abs))]) + '\\n')\n",
    "        f.close()\n",
    "        # **************************************************************************************************************信噪比直方图\n",
    "        figure(3 * filesNumInter + 1)\n",
    "        snrAlpha = 0.5\n",
    "        snrLinewidth = 0.8\n",
    "\n",
    "        for interI in range(filesNumInter):\n",
    "            sns.distplot(snrAllfile[interI], color=colorSpectrumColour[interI % len(colorSpectrumColour)],\n",
    "                         label=files[interI],\n",
    "                         # hist_kws={'alpha': snrAlpha},\n",
    "                         hist=False,\n",
    "                         # 不显示直方图\n",
    "                         # kde=False,\n",
    "                         # 不显示密度图\n",
    "                         kde_kws={'linewidth': snrLinewidth})\n",
    "        plt.title('SNR Density Curves with Histogram', fontsize=12)\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.xlabel(\"SNR\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        outPath5 = outPath + '/' + r'_snrHistogram.tif'\n",
    "        plt.savefig(outPath5, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath5 = outPath + '/' + r'_snrHistogram.pdf'\n",
    "        plt.savefig(outPath5, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # **************************************************************************************************************信噪比箱型图\n",
    "        figure(3 * filesNumInter + 2)\n",
    "\n",
    "        dfLists = []\n",
    "        # 存储以文件为单位信噪比字典\n",
    "        for interI in range(filesNumInter):\n",
    "            dfLists.append(pd.DataFrame({files[interI]: snrAllfile[interI]}))\n",
    "        dfData = pd.concat(dfLists, axis=1)\n",
    "\n",
    "        # color = dict(boxes='DarkGreen', whiskers='DarkOrange', medians='DarkBlue', caps='Gray')\n",
    "        #\n",
    "        # #字典拼接\n",
    "        # dfData.plot.box(color = color)\n",
    "        # dfData.boxplot(sym='+', patch_artist=True,\n",
    "        #                boxprops={\"color\": colorSpectrumColour[interI % 13], \"facecolor\": \"#F43D68\"},\n",
    "        #                meanprops={\"marker\": \"D\", \"markerfacecolor\": \"white\"}, medianprops={\"linestyle\": \"--\", \"color\": \"r\"})\n",
    "        # plt.grid(linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "        f = dfData.boxplot(sym='+', patch_artist=True, return_type='dict')\n",
    "\n",
    "        # 这里共有四个box\n",
    "        # color = ['k', 'g', 'r', 'deepskyblue']  # 有多少box就对应设置多少颜色\n",
    "\n",
    "        for box, c in zip(f['boxes'], colorSpectrumColour):\n",
    "            # 箱体边框颜色\n",
    "            box.set(color=c, linewidth=2)\n",
    "            # 箱体内部填充颜色\n",
    "            box.set(facecolor=c)\n",
    "\n",
    "        # plt.show()\n",
    "        # for interI in range(i):\n",
    "        #     sns.boxplot(x=classOneLabel[interI],y=snrAllfile[interI], notch=False)\n",
    "        # print(len(classLabel))\n",
    "\n",
    "        # print(\"111:\",np.array(snrAllfile).shape)\n",
    "        # df = pd.DataFrame(np.array(snrAllfile).T,\n",
    "        #                   columns=files)\n",
    "\n",
    "        # print(\"111:\",np.array(snrAllfile).shape)\n",
    "        # #数组重塑大小\n",
    "        # Data = pd.DataFrame(np.array(snrAllfile).reshape(1,3),columns = files)\n",
    "        # print(Data)\n",
    "        # Data.boxplot()\n",
    "        # plt.boxplot(snrAllfile, classLabel)\n",
    "        # 按照标签命名\n",
    "        # plt.boxplot(snrAllfile, classLabelName)\n",
    "        # print(classLabelName)\n",
    "        # notch=None,    # 是否是凹口的形式展现箱线图，默认非凹口；\n",
    "        # sym=None,    # 指定异常点的形状，默认为+号显示；\n",
    "        plt.grid(axis=\"y\", ls=\":\", lw=1, color=\"gray\", alpha=0.8)\n",
    "        outPath6 = outPath + '/' + r'_snrBox.tif'\n",
    "        plt.savefig(outPath6, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath6 = outPath + '/' + r'_snrBox.pdf'\n",
    "        plt.savefig(outPath6, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # **********************************************************************************************************拼接\n",
    "        allData = filesDataName[0]\n",
    "        for interI in range(1, filesNumInter):\n",
    "            # tempData = filesDataName[interI]\n",
    "            allData = np.column_stack((allData, (filesDataName[interI])[:, 1:]))\n",
    "        ySpec = allData[:, 1:allData.shape[1]].T\n",
    "\n",
    "        # **************************************************************************************************************3 hca层次聚类\n",
    "        figure(3 * filesNumInter + 3)\n",
    "        if distanceMethod == 0:\n",
    "            distM = \"ward\"\n",
    "            # 沃德方差最小化算法\n",
    "        elif distanceMethod == 1:\n",
    "            distM = \"centroid\"\n",
    "            # 质心间的欧式距离\n",
    "        elif distanceMethod == 2:\n",
    "            distM = \"weighted\"\n",
    "            # 加权分组平均法\n",
    "        else:\n",
    "            distM = 'average'\n",
    "            # UPGMA算法（非加权组平均）法\n",
    "\n",
    "        # distanceMethod = 'ward'\n",
    "        # # 层次聚类距离度量选用的方法,可选：'centroid','weighted','average'\n",
    "        Z = hierarchy.linkage(ySpec, method=distM, metric='euclidean')\n",
    "        # 执行层次/聚集聚类\n",
    "        hcaColorThreshold = 40\n",
    "        # color_threshold= 8 设置颜色高度阈值，用于控制决策树和PCA的颜色分类\n",
    "        # hcaLabelsize = 6\n",
    "        # #标号大小\n",
    "        # cluster.hierarchy.cut_tree(Z, n_clusters=filesNumInter)\n",
    "        # 设置聚类个数n_clusters\n",
    "\n",
    "        hierarchy.dendrogram(Z, color_threshold=hcaColorThreshold, labels=pd.DataFrame(ySpec).index + 1)\n",
    "        # 将分层聚类绘制为树状图\n",
    "        # hierarchy.dendrogram(Z, labels=pd.DataFrame(ySpec).index + 1)\n",
    "        # label = cluster.hierarchy.cut_tree(Z, height = hcaColorThreshold)\n",
    "        hacLeaves = hierarchy.leaves_list(Z) + 1\n",
    "        # 返回叶子节点\n",
    "\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tick_params(labelsize=hcaLabelsize)\n",
    "        # 调整刻度尺寸\n",
    "        plt.title('HCA')\n",
    "        # plt.show()\n",
    "        outPath7 = outPath + '/' + r'_HCA.tif'\n",
    "        plt.savefig(outPath7, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath7 = outPath + '/' + r'_HCA.pdf'\n",
    "        plt.savefig(outPath7, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "        # *******************************************************************************************保存层次聚类叶子节点\n",
    "        f = open(outPath + '/' + r'_hcaLeaves.txt', \"w\")\n",
    "        f.write(\"层次聚类树状图叶子节点（从左到右排列）：\")\n",
    "        f.write('\\n')\n",
    "        f.write(str(hacLeaves))\n",
    "        f.close()\n",
    "\n",
    "        # **************************************************************************************************************4 PCA数据压缩\n",
    "        figure()\n",
    "        # 根据两个最大的主成分进行绘图\n",
    "        pcaNcomponents = 4\n",
    "        # 主成分个数\n",
    "        pca = skldec.PCA(n_components=pcaNcomponents)\n",
    "        # 第一种，选择指定降维方差：n_components =0.95，贡献率之和到95%\n",
    "        # 第二种，选择指定降维后的维数：n_components =5\n",
    "        try:\n",
    "            pca.fit(ySpec)\n",
    "        except:\n",
    "            pca.fit(ySpec)\n",
    "        # pca.fit(ySpec)\n",
    "        # 主城分析时每一行是一个输入数据\n",
    "        result = pca.transform(ySpec)\n",
    "\n",
    "        # #**********************************************************************二维pca标记\n",
    "        #\n",
    "        tsneCount1 = 0\n",
    "        tsneCount2 = 0\n",
    "\n",
    "        # pcaSize = 30\n",
    "        # #pca标签大小\n",
    "        # pcaAlpha = 1\n",
    "        # #pca透明度\n",
    "        for tsneI in range(len(classOneLabel)):\n",
    "            # print(\"光谱名称：\",files[i])\n",
    "            # print(type(files[i]))\n",
    "            # print(\"光谱数量：\", filesDataName[i].shape)\n",
    "            tsneCount2 = tsneCount2 + pcaCount[tsneI]\n",
    "            #plt.scatter(result[tsneCount1:tsneCount2, pca_col1], result[tsneCount1:tsneCount2, pca_col2],\n",
    "            #            c=colorSpectrumColour[tsneI % len(colorSpectrumColour)], marker=markers[tsneI % len(markers)],\n",
    "            #            s=pcaSize,\n",
    "            #            alpha=pcaAlpha, edgecolor='k', label=files[tsneI])\n",
    "            plt.scatter(result[tsneCount1:tsneCount2, pca_col1], result[tsneCount1:tsneCount2, pca_col2],\n",
    "                        c=colorSpectrumColour[tsneI % len(colorSpectrumColour)], marker=markers[tsneI % len(markers)],\n",
    "                        s=pcaSize,\n",
    "                        alpha=pcaAlpha, label=files[tsneI])\n",
    "            tsneCount1 = tsneCount1 + pcaCount[tsneI]\n",
    "\n",
    "        # # 二维标记\n",
    "        # for i in range(0,len(hacLeaves)):\n",
    "        #     plt.text(result[i, 0], result[i, 1], i + 1, fontsize=10)\n",
    "        # plt.show()\n",
    "\n",
    "        plt.legend(loc=0, ncol=1, fontsize=8)\n",
    "        # plt.legend(loc=0, ncol=1, fontsize=5)\n",
    "        # set(tsneL, 'FontName', 'Times New Roman', 'FontSize', 5, 'FontWeight', 'normal')\n",
    "        plt.xlabel(u\"PC 1\")\n",
    "        plt.ylabel(u\"PC 2\")\n",
    "        plt.title('PCA_2D')\n",
    "        # plt.show()\n",
    "        outPath71 = outPath + '/' + r'_PCA_2D.tif'\n",
    "        plt.savefig(outPath71, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath71 = outPath + '/' + r'_PCA_2D.pdf'\n",
    "        plt.savefig(outPath71, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # # **************************************************************************************************二元关系分析\n",
    "        #\n",
    "        # 修改类别形状，由一维变二维（n,）变为(n,1),方便后续列拼接\n",
    "        classLabelName2 = np.array(classLabelName).reshape(-1, 1)\n",
    "        tsneClass = np.column_stack((result, classLabelName2))\n",
    "        # np.savetxt(r\"C:\\Users\\lixin\\Desktop\\b\\tsne.txt\",\n",
    "        #            result2, delimiter='\\t', fmt='%.2f')\n",
    "        outPuthExcPCA = outPath + '/' + r'PCA.csv'\n",
    "        # allDataTwoClass = np.column_stack(\n",
    "        #     (np.array(xLabelTest), np.array(yLabelTest), np.array(testTarget), np.array(knn.predict(testData)),\n",
    "        #      np.array(knn.predict(testData)), np.array(clf.predict(testData))))\n",
    "        # 存储前4维PCA\n",
    "        tsneList = pd.DataFrame(tsneClass, columns=['PC1', 'PC2', 'PC3', 'PC4', 'labels'])\n",
    "        tsneList.to_csv(outPuthExcPCA, index=False)\n",
    "        # 调用保存的tsne表格,index=False:不要序号\n",
    "        # df = pd.read_csv(outPuthExc)\n",
    "        # dfTSNE = pd.read_csv(outPuthExcTSNE).iloc[:,1:5]\n",
    "        # 切片\n",
    "        dfPCA = pd.read_csv(outPuthExcPCA)\n",
    "\n",
    "        g = sns.pairplot(dfPCA, kind=\"scatter\", diag_kind=\"kde\", diag_kws=dict(bw=1.5), hue=\"labels\", palette=\"hls\",\n",
    "                         plot_kws=dict(s=12, edgecolor=\"black\", linewidth=0.3))\n",
    "        # g = sns.pairplot(dfPCA, kind=\"scatter\", diag_kind=\"kde\", hue=\"labels\", palette=\"hls\",\n",
    "        #                  plot_kws=dict(s=15, edgecolor=\"black\", linewidth=0.5),kde_kws = {'bw' : 1.5})\n",
    "        # g.map_diag(sns.kdeplot)\n",
    "        # kind：用于控制非对角线上的图的类型，可选\"scatter\"与\"reg\"\n",
    "        # diag_kind：控制对角线上的图的类型，可选\"hist\"(直方图)与\"kde\"（密度图），加上diag_kws=dict(bw=1.5),避免报错\n",
    "        # hue ：针对某一字段进行分类\n",
    "        # palette：控制色调\n",
    "        g.map_offdiag(sns.kdeplot, n_levels=2)\n",
    "        # sns.kdeplot(x,y,cmap=pal)：绘制核密度分布图\n",
    "        # plt.show()\n",
    "        outPath91 = outPath + '/' + r'_pcaPair.tif'\n",
    "        plt.savefig(outPath91, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath91 = outPath + '/' + r'_pcaPair.pdf'\n",
    "        plt.savefig(outPath91, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ********************************************************************************************************PCA三维\n",
    "        figure(3 * filesNumInter + 4)\n",
    "        ax = plt.figure().add_subplot(111, projection='3d')\n",
    "        # 其中参数111，指的是将图像分成1行1列，此子图占据从左到右从上到下的1位置\n",
    "        pcaNcomponents3D = nDim + 1\n",
    "        # 主成分个数\n",
    "        pca_3D = skldec.PCA(n_components=pcaNcomponents3D)\n",
    "        # 第一种，选择指定降维方差：n_components =0.95，贡献率之和到95%\n",
    "        # 第二种，选择指定降维后的维数：n_components =5\n",
    "        # pca_3D.fit(ySpec)\n",
    "        try:\n",
    "            pca_3D.fit(ySpec)\n",
    "        except:\n",
    "            pca_3D.fit(ySpec)\n",
    "        # 主城分析时每一行是一个输入数据\n",
    "        result_3D = pca_3D.transform(ySpec)\n",
    "\n",
    "        pca_col3 = 2\n",
    "        pcaCount1 = 0\n",
    "        pcaCount2 = 0\n",
    "        # pcaSize = 30\n",
    "        # #pca标签大小\n",
    "        # pcaAlpha = 1\n",
    "        # #pca透明度\n",
    "        for pcai in range(len(classOneLabel)):\n",
    "            pcaCount2 = pcaCount2 + pcaCount[pcai]\n",
    "            ax.scatter(result_3D[pcaCount1:pcaCount2, pca_col1], result_3D[pcaCount1:pcaCount2, pca_col2],\n",
    "                       result_3D[pcaCount1:pcaCount2, pca_col3],\n",
    "                       c=colorSpectrumColour[pcai % len(colorSpectrumColour)], marker=markers[pcai % len(markers)],\n",
    "                       s=pcaSize,\n",
    "                       alpha=pcaAlpha, edgecolor='k', label=files[pcai])\n",
    "            pcaCount1 = pcaCount1 + pcaCount[pcai]\n",
    "        # 设置坐标轴\n",
    "        plt.legend(loc='best', fontsize=8)\n",
    "        ax.set_xlabel('PC1')\n",
    "        ax.set_ylabel('PC2')\n",
    "        ax.set_zlabel('PC3')\n",
    "        plt.title('PCA_3D')\n",
    "        # plt.show()\n",
    "        # return 0\n",
    "        outPath16 = outPath + '/' + r'_PCA_3D.tif'\n",
    "        plt.savefig(outPath16, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath16 = outPath + '/' + r'_PCA_3D.pdf'\n",
    "        plt.savefig(outPath16, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        ##**************************************************************************************************************tsne降维\n",
    "        figure(3 * filesNumInter + 5)\n",
    "        # 根据两个最大的主成分进行绘图\n",
    "        pcaNcomponents = 4\n",
    "        # 主成分个数\n",
    "        # pca = skldec.PCA(n_components=pcaNcomponents)\n",
    "        # # 第一种，选择指定降维方差：n_components =0.95，贡献率之和到95%\n",
    "        # # 第二种，选择指定降维后的维数：n_components =5\n",
    "        # pca.fit(ySpec)\n",
    "        # # 主城分析时每一行是一个输入数据\n",
    "        # result = pca.transform(ySpec)\n",
    "        nDim = 2\n",
    "        # int, 空间的维度\n",
    "        nPerplex = 10.0\n",
    "        # 数据集越大，需要参数值越大，建议值位5 - 50\n",
    "        learnRate = 200\n",
    "        # float,学习率，建议取值为10.0-1000.0\n",
    "        nInter = 1000\n",
    "        # int, 最大迭代次数\n",
    "        nGrad = 1e-06\n",
    "        # float,如果梯度低于该值，则停止算法\n",
    "\n",
    "        result2 = TSNE(n_components=nDim, perplexity=nPerplex, early_exaggeration=12.0, learning_rate=learnRate,\n",
    "                       n_iter=nInter,\n",
    "                       n_iter_without_progress=300, min_grad_norm=nGrad, metric='euclidean', init='pca',\n",
    "                       verbose=0, random_state=1, method='exact').fit_transform(ySpec)\n",
    "        # random_state:int, RandomState or None, default:None，伪随机数生成器种子,不同的初始化可能会导致成本函数的局部最小值不同\n",
    "\n",
    "        # print(type(result2))\n",
    "\n",
    "        # pca_col1 = 0\n",
    "        # pca_col2 = 1\n",
    "        tsneCount1 = 0\n",
    "        tsneCount2 = 0\n",
    "\n",
    "        # pcaSize = 30\n",
    "        # #pca标签大小\n",
    "        # pcaAlpha = 1\n",
    "        # #pca透明度\n",
    "        for tsneI in range(len(classOneLabel)):\n",
    "            # print(\"光谱名称：\",files[i])\n",
    "            # print(type(files[i]))\n",
    "            # print(\"光谱数量：\", filesDataName[i].shape)\n",
    "            tsneCount2 = tsneCount2 + pcaCount[tsneI]\n",
    "            plt.scatter(result2[tsneCount1:tsneCount2, pca_col1], result2[tsneCount1:tsneCount2, pca_col2],\n",
    "                        c=colorSpectrumColour[tsneI % len(colorSpectrumColour)], marker=markers[tsneI % len(markers)],\n",
    "                        s=pcaSize,\n",
    "                        alpha=pcaAlpha, label=files[tsneI])\n",
    "            tsneCount1 = tsneCount1 + pcaCount[tsneI]\n",
    "\n",
    "        # for i in range(100):\n",
    "        #     plt.text(result2[i, 0], result2[i, 1], i + 1, fontsize=14)\n",
    "        # plt.show()\n",
    "        plt.legend(loc=0, ncol=1, fontsize=8)\n",
    "        # plt.legend(loc=0, ncol=1, fontsize=5)\n",
    "        # set(tsneL, 'FontName', 'Times New Roman', 'FontSize', 5, 'FontWeight', 'normal')\n",
    "        plt.xlabel(u\"tSNE 1\")\n",
    "        plt.ylabel(u\"tSNE 2\")\n",
    "        plt.title('t-SNE')\n",
    "        # plt.show()\n",
    "        outPath9 = outPath + '/' + r'_tSNE.tif'\n",
    "        plt.savefig(outPath9, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath9 = outPath + '/' + r'_tSNE.pdf'\n",
    "        plt.savefig(outPath9, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ****************************************************************************************************************按文件split 分割 & 拼接\n",
    "        # Target0 = np.zeros(np.array(filesDataName[0][:, 1:]).shape[1])\n",
    "        # print(type(Target0))\n",
    "        # print(Target0)\n",
    "        Target0 = np.full(np.array(filesDataName[0][:, 1:]).shape[1], 0, dtype=int)\n",
    "        # 创建首个文件label,标签转置，转换成和数据同格式\n",
    "        # print(Target0)\n",
    "        # print((filesDataName[0][:, 1:].T).shape)\n",
    "        # print((Target0.T).shape)\n",
    "        trainData, testData, trainTarget, testTarget = train_test_split((filesDataName[0][:, 1:].T), (Target0.T),\n",
    "                                                                        test_size=svmTestSize, random_state=1)\n",
    "        # train_test_split交叉验证中常用的函数\n",
    "        # svmC = 0.8\n",
    "        # # 错误项的惩罚系数,范围[0.5-1],C越大分类效果越好，但有可能会过拟合（defaul C = 1）\n",
    "        # 首个文件数据分割\n",
    "        for interI in range(1, filesNumInter):\n",
    "            #\n",
    "            # TargetTep = np.zeros(np.array(filesDataName[interI][:, 1:]).shape[1])\n",
    "            TargetTep = np.full(np.array(filesDataName[interI][:, 1:]).shape[1], interI, dtype=int)\n",
    "            # print(TargetTep)\n",
    "            # 创建其他文件label#,标签转置，转换成和数据同格式\n",
    "            trainDataTep, testDataTep, trainTargetTep, testTargetTep = train_test_split(\n",
    "                (filesDataName[interI][:, 1:].T), TargetTep.T,\n",
    "                test_size=svmTestSize, random_state=1)\n",
    "            # 其他文件数据分割并拼接\n",
    "            trainData = np.row_stack((trainData, trainDataTep))\n",
    "            testData = np.row_stack((testData, testDataTep))\n",
    "            trainTarget = np.concatenate((trainTarget, trainTargetTep), axis=0)\n",
    "            # 一维数组垂直拼接\n",
    "            testTarget = np.concatenate((testTarget, testTargetTep), axis=0)\n",
    "            # 一维数组垂直拼接\n",
    "\n",
    "        # print(trainData.shape)\n",
    "        # print(testData.shape)\n",
    "        # print(trainTarget.shape)\n",
    "        # print(testTarget.shape)\n",
    "\n",
    "        # dataTarget1 = label_binarize(trainTarget, np.arange(len(classOneLabel)))\n",
    "        testTarget1 = label_binarize(testTarget, classes=np.arange(len(classOneLabel)))\n",
    "        # 标签二值化，为了将ROC曲线和ROC区域扩展到多类或多标签分类，有必要对输出进行二值化：\n",
    "        # ⑴可以每个标签绘制一条ROC曲线。\n",
    "        # ⑵也可以通过将标签指示符矩阵的每个元素视为二元预测（微平均）来绘制ROC曲线。\n",
    "        # ⑶另一种用于多类别分类的评估方法是宏观平均，它对每个标签的分类给予相同的权重。\n",
    "\n",
    "        # return 0\n",
    "        # trainData, testData, trainTarget, testTarget = train_test_split(ySpec, dataTarget,\n",
    "        #                                                                 test_size=svmTestSize, random_state=1)\n",
    "        # **************************************************************************************************************SVM混淆矩阵\n",
    "        figure(3 * filesNumInter + 6)\n",
    "        # 分类SVM、LDA、KNN\n",
    "        # print(\"数据大小\",ySpec.shape)\n",
    "        # classData = np.column_stack((ySpec, np.array(classLabel).T))\n",
    "        # 将标签拼接到数据中\n",
    "        # print(\"拼接标签后的数据大小：\",classData.shape)\n",
    "        # *********************************************************************************************网格搜索和交叉验证\n",
    "        # modelSVM = svm.SVC(C=svmC,probability=True)\n",
    "        modelSVM = svm.SVC(decision_function_shape='ovr', probability=True, class_weight='balanced')\n",
    "        # class_weight = {1: 10},class_weight='balanced'\n",
    "        # kernel='linear'时，为线性核，C越大分类效果越好，但有可能会过拟合（defaul C=1）。\n",
    "        # kernel='rbf'时（default），为高斯核，gamma值越小，分类界面越连续；\n",
    "        # gamma值越大，分类界面越“散”，分类效果越好，但有可能会过拟合。\n",
    "        # kernel='rbf'时（default），为高斯核，；\n",
    "\n",
    "        # 建立需要搜索的参数的范围\n",
    "        param_grid = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                      'C': [0.7, 0.8, 1]}\n",
    "        grid_search = GridSearchCV(modelSVM, param_grid, cv=3)\n",
    "        try:\n",
    "            clf = grid_search.fit(trainData, trainTarget)\n",
    "        except:\n",
    "            clf = grid_search.fit(trainData, trainTarget)\n",
    "        # clf = grid_search.fit(trainData, trainTarget)\n",
    "        # 训练模型\n",
    "\n",
    "        # *******************************************************************************************保存、加载分类器模型\n",
    "        # 保存模型\n",
    "        outPathSVM = outPath + '/' + r'_svmModel.pkl'\n",
    "        joblib.dump(clf, outPathSVM)\n",
    "        # 加载模型\n",
    "        # outPathSVM = outPath + '\\\\' + r'clfSVM.pkl'\n",
    "        # clfSVM = joblib.load(outPathSVM)\n",
    "        best_model_svm = grid_search.best_estimator_\n",
    "        # print(\"***\",type(grid_search.best_params_))\n",
    "        # print(type(grid_search.best_params_))\n",
    "        # print(\"默认超参数：\",best_model)\n",
    "        # print(type(best_model))\n",
    "        # # ********************************************************************************************保存SVM网格参数\n",
    "        f = open(outPath + '/' + r'_svmParameter.txt', \"w\")\n",
    "        f.write(\"SVM参数说明：\\n\")\n",
    "        f.write('搜索网格参数1.核函数(kernel)\\n')\n",
    "        f.write('\\t线性核函数(linear):主要用于线性可分，参数少，速度快\\n')\n",
    "        f.write('\\t径向基核函数(rbf):主要用于线性不可分，参数多,耗时\\n')\n",
    "        f.write('\\t多项式核函数(poly):依靠升维使得原本线性不可分的数据线性可分\\n')\n",
    "        f.write('\\tSigmoid核函数:SVM实现的就是一种多层感知器神经网络\\n')\n",
    "        f.write('搜索网格参数2.惩罚参数(C)\\n')\n",
    "        f.write('\\t[0.7, 0.8, 0.9, 1]\\n\\tC越大，松弛变量惩罚增大，即对误分类的惩罚增大，训练测试准确率高，但泛化能力弱；'\n",
    "                'C值小，对误分类的惩罚减小，允许容错，将他们当成噪声点，泛化能力较强。\\n')\n",
    "        f.write('搜索最优网格参数:' + '\\n' + str(grid_search.best_params_) + '\\n')\n",
    "        f.write('默认超参数:' + \"\\n\" + str(best_model_svm) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        y_scoreSVM1 = clf.predict(testData)\n",
    "        # 预测标签\n",
    "        y_scoreSVM = clf.predict_proba(testData)\n",
    "        # 预测概率，计算样本点到分割超平面的函数距离,各标签预测概率[0.8,0.2]\n",
    "\n",
    "        scoreSVM = clf.score(testData, testTarget)\n",
    "        # 测试数据准去率\n",
    "        # ORDER = [0, 1, 2, 3, 4]\n",
    "        # classOneLabel\n",
    "        matrixFindsize = 12\n",
    "        # 混淆矩阵字体大小\n",
    "        matrixDip = 120\n",
    "        # 分辨率\n",
    "        # Plot confusion matrix  绘图混淆矩阵\n",
    "        sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": matrixFindsize})\n",
    "        label = [files[i] for i in classOneLabel]\n",
    "        cm = confusion_matrix(testTarget, clf.predict(testData), labels=classOneLabel)\n",
    "        plt.figure(figsize=(8, 6),\n",
    "                   dpi=matrixDip,\n",
    "                   # 分辨率\n",
    "                   facecolor='snow',\n",
    "                   # 背景色,'snow','blueviolet','lightgray'\n",
    "                   )\n",
    "        # 设置title\n",
    "        plt.title('Test results of SVM classifier', fontsize=12)\n",
    "\n",
    "        cm = 100 * cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "        ax = sns.heatmap(cm, annot_kws={'size': 8},\n",
    "                         annot=True, cmap='Wistia', fmt='0.1f', xticklabels=label, yticklabels=label)\n",
    "        # cmap的参数如下：Wistia（蓝绿黄）\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        # plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        # 设置刻度值即坐标轴刻度 xticks yticks\n",
    "        plt.tick_params(labelsize=10)\n",
    "        # 设置坐标轴的刻度参数\n",
    "        outPath10 = outPath + '/' + r'_SVM.tif'\n",
    "        plt.savefig(outPath10, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath10 = outPath + '/' + r'_SVM.pdf'\n",
    "        plt.savefig(outPath10, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # ******************************************************************************************SVM的ROC曲线与AUC面积\n",
    "\n",
    "        if 2 == len(classOneLabel):\n",
    "            # 二分类ROC曲线\n",
    "            fpr, tpr, threshold = roc_curve(testTarget, y_scoreSVM1)\n",
    "            ###计算真正率和假正率\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ###计算auc的值\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "            ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('SVM_ROC', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            outPath101 = outPath + '/' + r'_SVM_ROC.tif'\n",
    "            plt.savefig(outPath101, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath101 = outPath + '/' + r'_SVM_ROC.pdf'\n",
    "            plt.savefig(outPath101, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            # # 多分类ROC曲线\n",
    "            # dataTarget1 = label_binarize(dataTarget, np.arange(len(classOneLabel)))\n",
    "            # # 标签二值化，为了将ROC曲线和ROC区域扩展到多类或多标签分类，有必要对输出进行二值化：\n",
    "            # # ⑴可以每个标签绘制一条ROC曲线。\n",
    "            # # ⑵也可以通过将标签指示符矩阵的每个元素视为二元预测（微平均）来绘制ROC曲线。\n",
    "            # # ⑶另一种用于多类别分类的评估方法是宏观平均，它对每个标签的分类给予相同的权重。\n",
    "            #\n",
    "            # print(np.array(dataTarget1).shape)\n",
    "            #\n",
    "            # trainData1, testData1, trainTarget1, testTarget1 = train_test_split(ySpec, dataTarget1,\n",
    "            #                                                                     test_size=svmTestSize, random_state=1)\n",
    "\n",
    "            # 计算每一类的ROC\n",
    "            fpr = dict()\n",
    "            tpr = dict()\n",
    "            roc_auc = dict()\n",
    "            # return 0\n",
    "            for i in range(len(classOneLabel)):\n",
    "                fpr[i], tpr[i], _ = roc_curve(testTarget1[:, i], y_scoreSVM[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area（方法二）\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget1.ravel(), y_scoreSVM.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "            # Compute macro-average ROC curve and ROC area（方法一）\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classOneLabel))]))\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(len(classOneLabel)):\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "            # Finally average it and compute AU\n",
    "            mean_tpr /= len(classOneLabel)\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label='micro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"micro\"]),\n",
    "                     color='deeppink', linestyle=':')\n",
    "\n",
    "            plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                     label='macro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"macro\"]),\n",
    "                     color='navy', linestyle=':')\n",
    "\n",
    "            # colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "            for i in range(len(classOneLabel)):\n",
    "                plt.plot(fpr[i], tpr[i],\n",
    "                         label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                               ''.format(files[i], roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('SVM_ROC:multi-class', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath101 = outPath + '/' + r'_SVM_ROC.tif'\n",
    "            plt.savefig(outPath101, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath101 = outPath + '/' + r'_SVM_ROC.pdf'\n",
    "            plt.savefig(outPath101, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        # ##**************************************************************************************************************LDA\n",
    "        figure(3 * filesNumInter + 7)\n",
    "        lda = LinearDiscriminantAnalysis()\n",
    "        # print('中心点',lda.means_)\n",
    "\n",
    "        # 建立需要搜索的参数的范围\n",
    "        param_grid = {'n_components': [1,2, 5, 10]}\n",
    "        grid_search_lda = GridSearchCV(lda, param_grid, cv=3)\n",
    "\n",
    "        try:\n",
    "            lda = grid_search_lda.fit(trainData, trainTarget)\n",
    "        except:\n",
    "            lda = grid_search_lda.fit(trainData, trainTarget)\n",
    "\n",
    "        # lda = grid_search_lda.fit(trainData, trainTarget)\n",
    "        # 训练模型\n",
    "        # *******************************************************************************************保存、加载分类器模型\n",
    "        # 保存模型\n",
    "        outPathLDA = outPath + '/' + r'_ldaModel.pkl'\n",
    "        joblib.dump(lda, outPathLDA)\n",
    "        # 加载模型\n",
    "        #\n",
    "        best_model_lda = grid_search_lda.best_estimator_\n",
    "        # print(\"***\",type(grid_search.best_params_))\n",
    "        # print(type(grid_search.best_params_))\n",
    "        # print(\"默认超参数：\",best_model)\n",
    "        # print(type(best_model))\n",
    "        # # ********************************************************************************************保存LDA网格参数\n",
    "        f = open(outPath + '/' + r'_ldaParameter.txt', \"w\")\n",
    "        f.write(\"LDA参数说明：\\n\")\n",
    "        f.write('搜索网格参数1.解释器(solver)\\n')\n",
    "        f.write('\\t奇异值分解(svd):对于有大规模特征的数据，推荐用这种算法\\n')\n",
    "        f.write('\\t最小平方差(lsqr):可以结合skrinkage参数\\n')\n",
    "        f.write('\\t特征分解算法(eigen):可以结合shrinkage参数\\n')\n",
    "\n",
    "        f.write('搜索网格参数2.指定了数组降维后的维度(n_components)\\n')\n",
    "        f.write('\\t[2,3,5,8,12]\\n')\n",
    "        f.write('搜索最优网格参数:' + '\\n' + str(grid_search_lda.best_params_) + '\\n')\n",
    "        f.write('默认超参数:' + \"\\n\" + str(best_model_lda) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        scoreLDA = lda.score(testData, testTarget)\n",
    "        # 测试数据准去率\n",
    "        # print('测试数据分类正确率', lda.score(X_test, Y_test))\n",
    "        y_scoreLDA1 = lda.predict(testData)\n",
    "        # 预测标签\n",
    "        y_scoreLDA = lda.predict_proba(testData)\n",
    "        # 预测标签概率\n",
    "\n",
    "        matrixFindsize = 12\n",
    "        # 混淆矩阵字体大小\n",
    "        matrixDip = 120\n",
    "        # 分辨率\n",
    "        # Plot confusion matrix  绘图混淆矩阵\n",
    "        sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": matrixFindsize})\n",
    "        label = [files[i] for i in classOneLabel]\n",
    "        cm = confusion_matrix(testTarget, lda.predict(testData), labels=classOneLabel)\n",
    "        plt.figure(figsize=(8, 6),\n",
    "                   dpi=matrixDip,\n",
    "                   # 分辨率\n",
    "                   facecolor='blueviolet',\n",
    "                   # 背景色,'snow','blueviolet','lightgray'\n",
    "                   )\n",
    "        # 设置title\n",
    "        plt.title('Test results of LDA classifier', fontsize=12)\n",
    "\n",
    "        cm = 100 * cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "        ax = sns.heatmap(cm, annot=True, cmap='YlOrRd', fmt='0.1f', xticklabels=label, yticklabels=label)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        # plt.xticks()\n",
    "        plt.yticks(rotation=0)\n",
    "        # 设置刻度值即坐标轴刻度 xticks yticks\n",
    "        plt.tick_params(labelsize=10)\n",
    "        # 设置坐标轴的刻度参数\n",
    "        outPath11 = outPath + '/' + r'_LDA.tif'\n",
    "        plt.savefig(outPath11, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath11 = outPath + '/' + r'_LDA.pdf'\n",
    "        plt.savefig(outPath11, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ******************************************************************************************LDA的ROC曲线与AUC面积\n",
    "\n",
    "        if 2 == len(classOneLabel):\n",
    "            # 二分类ROC曲线\n",
    "            fpr, tpr, threshold = roc_curve(testTarget, y_scoreLDA1)\n",
    "            ###计算真正率和假正率\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ###计算auc的值\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "            ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('LDA_ROC', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath111 = outPath + '/' + r'_LDA_ROC.tif'\n",
    "            plt.savefig(outPath111, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath111 = outPath + '/' + r'_LDA_ROC.pdf'\n",
    "            plt.savefig(outPath111, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            #\n",
    "            for i in range(len(classOneLabel)):\n",
    "                fpr[i], tpr[i], _ = roc_curve(testTarget1[:, i], y_scoreLDA[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area（方法二）\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget1.ravel(), y_scoreLDA.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "            # Compute macro-average ROC curve and ROC area（方法一）\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classOneLabel))]))\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(len(classOneLabel)):\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "            # Finally average it and compute AU\n",
    "            mean_tpr /= len(classOneLabel)\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label='micro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"micro\"]),\n",
    "                     color='deeppink', linestyle=':')\n",
    "\n",
    "            plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                     label='macro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"macro\"]),\n",
    "                     color='navy', linestyle=':')\n",
    "\n",
    "            # colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "            for i in range(len(classOneLabel)):\n",
    "                plt.plot(fpr[i], tpr[i],\n",
    "                         label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                               ''.format(files[i], roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('LDA_ROC:multi-class', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath111 = outPath + '/' + r'_LDA_ROC.tif'\n",
    "            plt.savefig(outPath111, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath111 = outPath + '/' + r'_LDA_ROC.pdf'\n",
    "            plt.savefig(outPath111, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        # **************************************************************************************************************KNN\n",
    "        figure(3 * filesNumInter + 8)\n",
    "\n",
    "        knn = KNeighborsClassifier()\n",
    "\n",
    "        # 定义一个knn分类器对象\n",
    "        # int 型参数 knn算法中指定以最近的几个最近邻样本具有投票权，默认参数为5，n_neighbors=5\n",
    "        # str参数,即每个拥有投票权的样本是按什么比重投票，'uniform'表示等比重投票，\n",
    "        # 'distance'表示按距离反比投票，[callable]表示自己定义的一个函数\n",
    "        # str或者距离度量对象   即怎样度量距离,默认是闵氏距离。\n",
    "\n",
    "        # 建立需要搜索的参数的范围\n",
    "        param_grid = {'n_neighbors': [3, 5, 7, 9], 'weights': ['uniform', 'distance'], 'leaf_size': [20, 30, 40]}\n",
    "        grid_search_knn = GridSearchCV(knn, param_grid, cv=3)\n",
    "        try:\n",
    "            knn = grid_search_knn.fit(trainData, trainTarget)\n",
    "        except:\n",
    "            knn = grid_search_knn.fit(trainData, trainTarget)\n",
    "        # knn = grid_search_knn.fit(trainData, trainTarget)\n",
    "        # 训练模型\n",
    "        # *******************************************************************************************保存、加载分类器模型\n",
    "        # 保存模型\n",
    "        outPathKNN = outPath + '/' + r'_knnModel.pkl'\n",
    "        joblib.dump(knn, outPathKNN)\n",
    "        # 加载模型\n",
    "        #\n",
    "        best_model_knn = grid_search_knn.best_estimator_\n",
    "        # print(\"***\",type(grid_search.best_params_))\n",
    "        # print(type(grid_search.best_params_))\n",
    "        # print(\"默认超参数：\",best_model)\n",
    "        # print(type(best_model))\n",
    "        # # ********************************************************************************************保存knn网格参数\n",
    "        f = open(outPath + '/' + r'_knnParameter.txt', \"w\")\n",
    "        f.write(\"knn参数说明：\\n\")\n",
    "        f.write('搜索网格参数1.相邻节点数(n_neighbors)\\n')\n",
    "        f.write('\\t[3,5,7,10]\\n')\n",
    "        f.write('搜索网格参数2.权重函数(weights)\\n')\n",
    "        f.write('\\tuniform:统一的权重. 在每一个邻居区域里的点的权重都是一样的。\\n')\n",
    "        f.write('\\tdistance:权重点等于他们距离的倒数。使用此函数，更近的邻居对于所预测的点的影响更大。\\n')\n",
    "        f.write('搜索网格参数3.叶子数量(leaf_size)\\n')\n",
    "        f.write('\\t[20,30,40]\\n')\n",
    "        f.write('搜索最优网格参数:' + '\\n' + str(grid_search_knn.best_params_) + '\\n')\n",
    "        f.write('默认超参数:' + \"\\n\" + str(best_model_knn) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        # knn.fit(trainData, trainTarget)\n",
    "        scoreKNN = knn.score(testData, testTarget)\n",
    "        # 测试数据准确率\n",
    "        y_scoreKNN1 = knn.predict(testData)\n",
    "        # 预测标签\n",
    "        y_scoreKNN = knn.predict_proba(testData)\n",
    "        # 预测标签概率\n",
    "        matrixFindsize = 12\n",
    "        # 混淆矩阵字体大小\n",
    "        matrixDip = 120\n",
    "        # 分辨率\n",
    "        # Plot confusion matrix  绘图混淆矩阵\n",
    "        sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": matrixFindsize})\n",
    "        label = [files[i] for i in classOneLabel]\n",
    "        cm = confusion_matrix(testTarget, knn.predict(testData), labels=classOneLabel)\n",
    "        plt.figure(figsize=(8, 6),\n",
    "                   dpi=matrixDip,\n",
    "                   # 分辨率\n",
    "                   facecolor='blueviolet',\n",
    "                   # 背景色,'snow','blueviolet','lightgray'\n",
    "                   )\n",
    "        # 设置title\n",
    "        plt.title('Test results of KNN classifier', fontsize=12)\n",
    "\n",
    "        cm = 100 * cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "        ax = sns.heatmap(cm, annot=True, cmap='YlGnBu', fmt='0.1f', xticklabels=label, yticklabels=label)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        # plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        # 设置刻度值即坐标轴刻度 xticks yticks\n",
    "        plt.tick_params(labelsize=10)\n",
    "        # 设置坐标轴的刻度参数\n",
    "        outPath12 = outPath + '/' + r'_KNN.tif'\n",
    "        plt.savefig(outPath12, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath12 = outPath + '/' + r'_KNN.pdf'\n",
    "        plt.savefig(outPath12, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # ******************************************************************************************KNN的ROC曲线与AUC面积\n",
    "        if 2 == len(classOneLabel):\n",
    "            # 二分类ROC曲线\n",
    "            fpr, tpr, threshold = roc_curve(testTarget, y_scoreKNN1)\n",
    "            ###计算真正率和假正率\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ###计算auc的值\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "            ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('KNN_ROC', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath121 = outPath + '/' + r'_KNN_ROC.tif'\n",
    "            plt.savefig(outPath121, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath121 = outPath + '/' + r'_KNN_ROC.pdf'\n",
    "            plt.savefig(outPath121, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            # 多分类\n",
    "            for i in range(len(classOneLabel)):\n",
    "                fpr[i], tpr[i], _ = roc_curve(testTarget1[:, i], y_scoreKNN[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area（方法二）\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget1.ravel(), y_scoreKNN.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "            # Compute macro-average ROC curve and ROC area（方法一）\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classOneLabel))]))\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(len(classOneLabel)):\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "            # Finally average it and compute AU\n",
    "            mean_tpr /= len(classOneLabel)\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label='micro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"micro\"]),\n",
    "                     color='deeppink', linestyle=':')\n",
    "\n",
    "            plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                     label='macro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"macro\"]),\n",
    "                     color='navy', linestyle=':')\n",
    "\n",
    "            # colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "            for i in range(len(classOneLabel)):\n",
    "                plt.plot(fpr[i], tpr[i],\n",
    "                         label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                               ''.format(files[i], roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('KNN_ROC:multi-class', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath121 = outPath + '/' + r'_KNN_ROC.tif'\n",
    "            plt.savefig(outPath121, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath121 = outPath + '/' + r'_KNN_ROC.pdf'\n",
    "            plt.savefig(outPath121, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # **************************************************************************************************************XGBoost\n",
    "        figure(3 * filesNumInter + 8)\n",
    "\n",
    "        xgb = XGBClassifier()\n",
    "        # scale_pos_weight,正样本的权重，在二分类任务中，当正负样本比例失衡时，设置正样本的权重，模型效果更好。\n",
    "        # 建立需要搜索的参数的范围\n",
    "\n",
    "        param_grid = {'max_depth': [5, 6, 7]}\n",
    "        grid_search_xgb = GridSearchCV(xgb, param_grid, cv=3)\n",
    "        try:\n",
    "            xgb = grid_search_xgb.fit(trainData, trainTarget)\n",
    "        except:\n",
    "            xgb = grid_search_xgb.fit(trainData, trainTarget)\n",
    "        # xgb = grid_search_xgb.fit(trainData, trainTarget)\n",
    "        # 训练模型\n",
    "        # *******************************************************************************************保存、加载分类器模型\n",
    "        # 保存模型\n",
    "        outPathXGB = outPath + '/' + r'_xgbModel.pkl'\n",
    "        joblib.dump(xgb, outPathXGB)\n",
    "        # 加载模型\n",
    "        best_model_xgb = grid_search_xgb.best_estimator_\n",
    "        # 网格函数选出的最优模型\n",
    "        # print(\"***\",type(grid_search.best_params_))\n",
    "        # print(type(grid_search.best_params_))\n",
    "        # print(\"默认超参数：\",best_model)\n",
    "        # print(type(best_model))\n",
    "        # **************************************************************************************************************XGB特征重要性(gain增益)排序条形图\n",
    "        '''\n",
    "        # 重要性指标feature_importances_，这个参数好像只有在决策树和以决策树为基础的算法有。\n",
    "        '''\n",
    "        top_K = 20\n",
    "        # 返回前n个特征\n",
    "        # xgbFeatureImportance = XGBClassifier()\n",
    "        xgbFeaImp = best_model_xgb.fit(trainData, trainTarget).feature_importances_\n",
    "        top_k_idx = np.array(xgbFeaImp).argsort()[::-1][0:top_K]\n",
    "        # argsort:返回的是数组值从小到大的索引值\n",
    "        f = open(outPath + '/' + r'_xgbGain.txt', \"w\")\n",
    "        f.write('用XGBoost模型对光谱特征重要性进行排序：输出前10最重要拉曼位移(特征)及重要性占比' + '\\n')\n",
    "        for interI in range(top_K):\n",
    "            f.write(\"拉曼位移:\" + str(x[top_k_idx[interI]]) + \", 占比：\" + str(round(xgbFeaImp[top_k_idx[interI]], 3)) + '\\n')\n",
    "\n",
    "        f.write('注：属性(拉曼位移对应的波数点)对分裂点改进性能度量越大（越靠近根节点），权值越大；\\n'\n",
    "                '被越多提升树所选择，属性越重要。这里性能度量选择分裂节点的gain增益。' + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        wavenumberX = np.arange(len(top_k_idx))\n",
    "        # 添加波数坐标\n",
    "        plt.bar(x=wavenumberX, height=xgbFeaImp[top_k_idx], width=0.5, tick_label=x[top_k_idx], color='#00BFFF')\n",
    "        plt.xticks(fontsize=9, rotation=45)\n",
    "        plt.yticks(fontsize=10)\n",
    "        plt.xlabel(\"Characteristic peak position\", fontsize=12)\n",
    "        plt.ylabel(\"Gain\", fontsize=12)\n",
    "        plt.tight_layout()\n",
    "        # 解决显示不全的问题\n",
    "        # plt.show()\n",
    "\n",
    "        plt.gca().yaxis.set_major_formatter(FuncFormatter(to_percent))\n",
    "        # from matplotlib import ticker\n",
    "        # plt.gca().yaxis.set_major_formatter(ticker.PercentFormatter(xmax=1, decimals=1))\n",
    "\n",
    "        # 纵坐标以百分百%显示\n",
    "        outPath123 = outPath + '/' + r'_XGB_Gain.tif'\n",
    "        plt.savefig(outPath123, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath123 = outPath + '/' + r'_XGB_Gain.pdf'\n",
    "        plt.savefig(outPath123, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "        # return 0\n",
    "        # # ********************************************************************************************保存xgb网格参数\n",
    "        f = open(outPath + '/' + r'_xgbParameter.txt', \"w\")\n",
    "        f.write(\"xgb参数说明：\\n\")\n",
    "        f.write('搜索网格参数1.基分类器模型(booster)\\n')\n",
    "        f.write('\\tgbtree:基分类器为树模型;gbliner:基分类器为线性模型\\n')\n",
    "        f.write('搜索网格参数2.树的深度(max_depth)\\n')\n",
    "        f.write('\\t[5,6,7]值过大容易过拟合，值过小容易欠拟合\\n')\n",
    "        f.write('\\tdistance:权重点等于他们距离的倒数。使用此函数，更近的邻居对于所预测的点的影响更大。\\n')\n",
    "        f.write('搜索网格参数3.基分类器的个数(n_estimatores)\\n')\n",
    "        f.write('\\t[80, 100, 120]\\n')\n",
    "        f.write('搜索最优网格参数:' + '\\n' + str(grid_search_xgb.best_params_) + '\\n')\n",
    "        f.write('默认超参数:' + \"\\n\" + str(best_model_xgb) + '\\n')\n",
    "        f.close()\n",
    "\n",
    "        # xgb.fit(trainData, trainTarget)\n",
    "        scoreXGB = xgb.score(testData, testTarget)\n",
    "        # 测试数据准确率\n",
    "        y_scoreXGB1 = xgb.predict(testData)\n",
    "        # 预测标签\n",
    "        y_scoreXGB = xgb.predict_proba(testData)\n",
    "        # 预测标签概率\n",
    "        matrixFindsize = 12\n",
    "        # 混淆矩阵字体大小\n",
    "        matrixDip = 120\n",
    "        # 分辨率\n",
    "        # Plot confusion matrix  绘图混淆矩阵\n",
    "        sns.set_context(\"talk\", rc={\"font\": \"Helvetica\", \"font.size\": matrixFindsize})\n",
    "        label = [files[i] for i in classOneLabel]\n",
    "        cm = confusion_matrix(testTarget, xgb.predict(testData), labels=classOneLabel)\n",
    "        plt.figure(figsize=(8, 6),\n",
    "                   dpi=matrixDip,\n",
    "                   # 分辨率\n",
    "                   facecolor='blueviolet',\n",
    "                   # 背景色,'snow','blueviolet','lightgray'\n",
    "                   )\n",
    "        # 设置title\n",
    "        plt.title('Test results of XGB classifier', fontsize=12)\n",
    "\n",
    "        cm = 100 * cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "        ax = sns.heatmap(cm, annot=True, cmap='YlGnBu', fmt='0.1f', xticklabels=label, yticklabels=label)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "        # plt.xticks(rotation=90)\n",
    "        plt.yticks(rotation=0)\n",
    "        # 设置刻度值即坐标轴刻度 xticks yticks\n",
    "        plt.tick_params(labelsize=10)\n",
    "        # 设置坐标轴的刻度参数\n",
    "        outPath122 = outPath + '/' + r'_XGB.tif'\n",
    "        plt.savefig(outPath122, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        outPath122 = outPath + '/' + r'_XGB.pdf'\n",
    "        plt.savefig(outPath122, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # *************************************************************************************XGB的ROC曲线与AUC面积\n",
    "        if 2 == len(classOneLabel):\n",
    "            # 二分类ROC曲线\n",
    "            fpr, tpr, threshold = roc_curve(testTarget, y_scoreXGB1)\n",
    "            ###计算真正率和假正率\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            ###计算auc的值\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            lw = 2\n",
    "            plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "            ###假正率为横坐标，真正率为纵坐标做曲线\n",
    "            plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('XGB_ROC', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath121 = outPath + '/' + r'_XGB_ROC.tif'\n",
    "            plt.savefig(outPath121, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath121 = outPath + '/' + r'_XGB_ROC.pdf'\n",
    "            plt.savefig(outPath121, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "        else:\n",
    "            # 多分类\n",
    "            for i in range(len(classOneLabel)):\n",
    "                fpr[i], tpr[i], _ = roc_curve(testTarget1[:, i], y_scoreXGB[:, i])\n",
    "                roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "            # Compute micro-average ROC curve and ROC area（方法二）\n",
    "            fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(testTarget1.ravel(), y_scoreXGB.ravel())\n",
    "            roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "            # Compute macro-average ROC curve and ROC area（方法一）\n",
    "            # First aggregate all false positive rates\n",
    "            all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classOneLabel))]))\n",
    "            # Then interpolate all ROC curves at this points\n",
    "            mean_tpr = np.zeros_like(all_fpr)\n",
    "            for i in range(len(classOneLabel)):\n",
    "                mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "            # Finally average it and compute AU\n",
    "            mean_tpr /= len(classOneLabel)\n",
    "            fpr[\"macro\"] = all_fpr\n",
    "            tpr[\"macro\"] = mean_tpr\n",
    "            roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "            # Plot all ROC curves\n",
    "            plt.figure()\n",
    "            plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
    "                     label='micro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"micro\"]),\n",
    "                     color='deeppink', linestyle=':')\n",
    "\n",
    "            plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
    "                     label='macro-average ROC curve (AUC = {0:0.2f})'\n",
    "                           ''.format(roc_auc[\"macro\"]),\n",
    "                     color='navy', linestyle=':')\n",
    "\n",
    "            # colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
    "            for i in range(len(classOneLabel)):\n",
    "                plt.plot(fpr[i], tpr[i],\n",
    "                         label='ROC curve of class {0} (AUC = {1:0.2f})'\n",
    "                               ''.format(files[i], roc_auc[i]))\n",
    "\n",
    "            plt.plot([0, 1], [0, 1], 'k--')\n",
    "            plt.xlim([0.0, 1.0])\n",
    "            plt.ylim([0.0, 1.05])\n",
    "            plt.xlabel('False Positive Rate', fontsize=12)\n",
    "            plt.ylabel('True Positive Rate', fontsize=12)\n",
    "            plt.title('XGB_ROC:multi-class', fontsize=12)\n",
    "            plt.legend(loc=\"lower right\", fontsize=8)\n",
    "            # plt.show()\n",
    "            outPath121 = outPath + '/' + r'_XGB_ROC.tif'\n",
    "            plt.savefig(outPath121, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath121 = outPath + '/' + r'_XGB_ROC.pdf'\n",
    "            plt.savefig(outPath121, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "        # **************************************************************************************************************回归分析画图(小于5类)\n",
    "\n",
    "        if filesNumInter <= 4:\n",
    "\n",
    "            # ySpecL = pd.DataFrame(ySpec)\n",
    "            # dataTargetL = pd.Series(dataTarget)\n",
    "            # # 数据类型转换\n",
    "            # trainDataL, testDataL, trainTargetL, testTargetL = train_test_split(ySpecL, dataTargetL,\n",
    "            #                                                                     test_size=svmTestSize, random_state=1)\n",
    "            # # 保证随机种子一致random_state=1\n",
    "            testTargetL = pd.Series(testTarget)\n",
    "            labelTest = list(testTargetL.index)\n",
    "            # labelTest = list(testTarget.index)\n",
    "\n",
    "            # labelTest.sort()\n",
    "            # # 测试数据索引标签排序\n",
    "            xLabelTest = []\n",
    "            yLabelTest = []\n",
    "            for interLabelTest in labelTest:\n",
    "                xLabelTest.append(result2[interLabelTest, pca_col1])\n",
    "                yLabelTest.append(result2[interLabelTest, pca_col2])\n",
    "\n",
    "            outPuthExc = outPath + '/' + r'Classification.csv'\n",
    "            allDataTwoClass = np.column_stack(\n",
    "                (np.array(xLabelTest), np.array(yLabelTest), np.array(testTarget), np.array(knn.predict(testData)),\n",
    "                 np.array(knn.predict(testData)), np.array(clf.predict(testData))))\n",
    "            allDataTwoClass1 = pd.DataFrame(allDataTwoClass,\n",
    "                                            columns=['x_tSNE1', 'y_tSNE2', 'trueLabel', 'svmClassify', 'ldaClassify',\n",
    "                                                     'knnClassify'])\n",
    "            allDataTwoClass1.to_csv(outPuthExc)\n",
    "\n",
    "            df = pd.read_csv(outPuthExc)\n",
    "            # **************************************************************************************************SVM二分类\n",
    "            sns.set_style(\"white\")\n",
    "            gridobj = sns.lmplot(x=\"x_tSNE1\", y=\"y_tSNE2\", hue=\"svmClassify\", data=df,\n",
    "                                 height=5, aspect=1.6, palette='tab10',\n",
    "                                 scatter_kws=dict(s=20, linewidths=.7, edgecolors='black'))\n",
    "            # gridobj = sns.lmplot(x=\"x_tSNE1\", y=\"y_tSNE2\", hue=\"svmClassify\", data=df,\n",
    "            #                      height=5, aspect=1.6, robust=True, palette='tab10',\n",
    "            #                      scatter_kws=dict(s=20, linewidths=.7, edgecolors='black'))\n",
    "            # Decorations\n",
    "            # gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))\n",
    "            gridobj.set()\n",
    "            plt.title(\"Scatterplot with line of best fit - SVM\", fontsize=10)\n",
    "            # plt.show()\n",
    "            outPath13 = outPath + '/' + r'_TwoCategoriesSVM.tif'\n",
    "            plt.savefig(outPath13, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath13 = outPath + '/' + r'_TwoCategoriesSVM.pdf'\n",
    "            plt.savefig(outPath13, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            # **************************************************************************************************LDA二分类\n",
    "            sns.set_style(\"white\")\n",
    "            gridobj = sns.lmplot(x=\"x_tSNE1\", y=\"y_tSNE2\", hue=\"ldaClassify\", data=df,\n",
    "                                 height=5, aspect=1.6, palette='tab10',\n",
    "                                 scatter_kws=dict(s=20, linewidths=.7, edgecolors='black'))\n",
    "\n",
    "            # Decorations\n",
    "            # gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))\n",
    "            gridobj.set()\n",
    "            plt.title(\"Scatterplot with line of best fit - LDA\", fontsize=10)\n",
    "            # plt.show()\n",
    "            outPath14 = outPath + '/' + r'_TwoCategoriesLDA.tif'\n",
    "            plt.savefig(outPath14, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath14 = outPath + '/' + r'_TwoCategoriesLDA.pdf'\n",
    "            plt.savefig(outPath14, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close()\n",
    "\n",
    "            # **************************************************************************************************KNN二分类\n",
    "            sns.set_style(\"white\")\n",
    "            gridobj = sns.lmplot(x=\"x_tSNE1\", y=\"y_tSNE2\", hue=\"knnClassify\", data=df,\n",
    "                                 height=5, aspect=1.6, palette='tab10',\n",
    "                                 scatter_kws=dict(s=20, linewidths=.7, edgecolors='black'))\n",
    "            # Decorations\n",
    "            # gridobj.set(xlim=(0.5, 7.5), ylim=(0, 50))\n",
    "            gridobj.set()\n",
    "            plt.title(\"Scatterplot with line of best fit - KNN\", fontsize=10)\n",
    "            # plt.show()\n",
    "            outPath15 = outPath + '/' + r'_TwoCategoriesKNN.tif'\n",
    "            plt.savefig(outPath15, format='tif', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            outPath15 = outPath + '/' + r'_TwoCategoriesKNN.pdf'\n",
    "            plt.savefig(outPath15, format='pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "            plt.close(\"all\")\n",
    "        # ********************************************************************************************************************写入word\n",
    "        # 输出信息\n",
    "        dataNum = allData.shape[1] - 1\n",
    "        # 各标签数据量\n",
    "        # outPutInfo = [dataNum, files, pcaCountAll, snrDelfile]\n",
    "\n",
    "        document = Document()\n",
    "        # 打开文档\n",
    "        document.styles['Normal'].font.name = u'宋体'\n",
    "        document.styles['Normal']._element.rPr.rFonts.set(qn('w:eastAsia'), u'宋体')\n",
    "        # 设置文档格式\n",
    "        # t0：标题\n",
    "        t0 = document.add_heading(u'实验报告', 1)\n",
    "        t0.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "        # 直接添加标题，“1”表示1级标题,标题居中\n",
    "        # t1,t2,t3次级标题\n",
    "        t1 = document.add_heading(u'1.数据信息', 2)\n",
    "        # 增加表格1\n",
    "        rows_num = 3\n",
    "        cols_num = filesNumInter + 1\n",
    "        table1 = document.add_table(rows=rows_num, cols=cols_num, style='Table Grid')\n",
    "        table1.style.font.name = u'宋体'\n",
    "        listTable1 = ['标签名', '数据量', '剔除光谱编号']\n",
    "        # 列标签\n",
    "        for i in range(rows_num):\n",
    "            table1.cell(i, 0).text = listTable1[i]\n",
    "        # 行信息\n",
    "        for j in range(1, cols_num):\n",
    "            table1.cell(0, j).text = files[j - 1]\n",
    "            table1.cell(1, j).text = str(pcaCountAll[j - 1])\n",
    "            table1.cell(2, j).text = str(snrDelfile[j - 1])\n",
    "        t2 = document.add_heading(u'2.参数信息', 2)\n",
    "        # 增加表格2\n",
    "        listTable2 = [\"参数名\", \"参数意义\", \"参数值\"]\n",
    "        listTable2Name = [\"dataPath\", \"outPath\", \"startSpec\", \"endSpec\", \"snrNoiseindex\", \"delSNRmin\", \"delSNRmax\",\n",
    "                          \"filterSize\",\n",
    "                          \"dynamicFactor\", \"winSG\", \"nSG\", \"lambdaAirPls\", \"itermaxAirPls\", \"pltInter\",\n",
    "                          \"processLinewidth\",\n",
    "                          \"distanceMethod\", \"hcaLabelsize\", \"pcaNcomponents\", \"pca_col1\", \"pca_col2\", \"pcaSize\",\n",
    "                          \"pcaAlpha\",\n",
    "                          \"nDim\", \"nPerplex\", \"learnRate\", \"nInter\", \"nGrad\", \"svmTestSize\", \"svmC\", \"xKernel\",\n",
    "                          \"clusters_N\"]\n",
    "        listTable2Meaning = [\"输入路径\", \"输出路径\", \"截谱长度(首)\", \"截谱长度(尾)\", \"静默区：尾端长度\", \"信噪比最小阈值\", \"信噪比最大阈值\",\n",
    "                             \"宇宙射线：窗口宽度\", \"宇宙射线：动态因子\", \"滤波：窗口宽度\", \"滤波：拟合阶次\",\n",
    "                             \"基线校正：lambda\", \"基线校正：迭代次数\", \"显示光谱量(未使用)\", \"光谱线宽(未使用)\",\n",
    "                             \"HCA距离度量方法:\\r0:沃德方差最小化\\r1:质心欧式距离\\r2:加权分组平均\\r3:UPGMA算法\",\n",
    "                             \"HCA(tSNE)标号大小\", \"PCA主成分个数\", \"PCA(tSNE)选择维度\", \"PCA(tSNE)选择维度\",\n",
    "                             \"PCA(tSNE)标签大小\", \"PCA(tSNE)透明度\", \"tSNE空间维度\", \"Perplex\", \"tSNE学习率\",\n",
    "                             \"tSNE最大迭代次数\", \"tSNE收敛误差\", \"测试数据占比\", \"SVM错误项惩罚系数\",\n",
    "                             \"SVM核函数:\\r0:多项式核\\r1:线性核\\r2:径向基核\\r3:sigmod核\\r4:核矩阵\", \"聚类堆数\"]\n",
    "        listTable2Value = [dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize,\n",
    "                           dynamicFactor, winSG,\n",
    "                           nSG, lambdaAirPls, itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize,\n",
    "                           pcaNcomponents, pca_col1, pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter,\n",
    "                           nGrad,\n",
    "                           svmTestSize, svmC, xKernel, clusters_N]\n",
    "        rows_num = len(listTable2Meaning) + 1\n",
    "        cols_num = len(listTable2)\n",
    "        table2 = document.add_table(rows=rows_num, cols=cols_num, style='Table Grid')\n",
    "        table2.style.font.name = u'宋体'\n",
    "        # 列标题\n",
    "        for j in range(0, cols_num):\n",
    "            table2.cell(0, j).text = listTable2[j]\n",
    "        # 行标题\n",
    "        for i in range(1, rows_num):\n",
    "            table2.cell(i, 0).text = listTable2Name[i - 1]\n",
    "            table2.cell(i, 1).text = listTable2Meaning[i - 1]\n",
    "            table2.cell(i, 2).text = str(listTable2Value[i - 1])\n",
    "        # 均值方差图\n",
    "        t3 = document.add_heading(u'3.均值方差图', 2)\n",
    "        for i in range(filesNumInter):\n",
    "            document.add_heading(files[i] + '_光谱均值_信噪比标准差', 3)\n",
    "            img_name = outPath + '/' + files[i] + r'_meanStd.tif'\n",
    "            fig2 = document.add_picture(img_name, width=Inches(5.0))\n",
    "            # 向文档里添加图片\n",
    "            # fig2.alignment = WD_PARAGRAPH_ALIGNMENT.CENTER\n",
    "            # #居中显示\n",
    "        # 均值方差罗列图\n",
    "        t4 = document.add_heading(u'4.均值方差罗列图', 2)\n",
    "        img_name = outPath + '/' + r'_meanStdcolumn.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # 信噪比箱型图\n",
    "        t5 = document.add_heading(u'5.信噪比箱型图', 2)\n",
    "        img_name = outPath + '/' + r'_snrBox.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # 信噪比直方图\n",
    "        t6 = document.add_heading(u'6.信噪比直方图', 2)\n",
    "        img_name = outPath + '/' + r'_snrHistogram.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # PCA\n",
    "        t7 = document.add_heading(u'7.PCA主成分分布图', 2)\n",
    "        img_name = outPath + '/' + r'_PCA_3D.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # tSNE\n",
    "        t8 = document.add_heading(u'8.tSNE空间分布图', 2)\n",
    "        img_name = outPath + '/' + r'_tSNE.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # HCA\n",
    "        t9 = document.add_heading(u'9.层次聚类树状图', 2)\n",
    "        img_name = outPath + '/' + r'_HCA.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # SV\n",
    "        t10 = document.add_heading(u'10.混淆矩阵：SVM测试结果', 2)\n",
    "        table10 = document.add_table(rows=1, cols=2, style='Table Grid')\n",
    "        table10.style.font.name = u'宋体'\n",
    "        table10.cell(0, 0).text = \"SVM测试准确率\"\n",
    "        table10.cell(0, 1).text = str(\"%.2f%%\" % (scoreSVM * 100))\n",
    "        img_name = outPath + '/' + r'_SVM.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # SVM_ROC_AUC\n",
    "        t11 = document.add_heading(u'10.ROC评估：SVM的ROC曲线及AUC值', 2)\n",
    "        table11 = document.add_table(rows=3, cols=2, style='Table Grid')\n",
    "        table11.style.font.name = u'宋体'\n",
    "        table11.cell(0, 0).text = \"macro-average ROC curve\"\n",
    "        table11.cell(0, 1).text = \"类别维度，n个类别ROC曲线均值\"\n",
    "        table11.cell(1, 0).text = \"micro-average ROC curve\"\n",
    "        table11.cell(1, 1).text = \"样本维度，计算所有测试样本ROC曲线均值\"\n",
    "        table11.cell(2, 0).text = \"ROC curve of class * \"\n",
    "        table11.cell(2, 1).text = \"类别 * 与SVM分类器预测概率绘制的ROC曲线\"\n",
    "        img_name = outPath + '/' + r'_SVM_ROC.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # SVM两类\n",
    "        if filesNumInter <= 4:\n",
    "            t101 = document.add_heading(u'10(2).最佳回归线散点图：SVM测试结果', 2)\n",
    "            img_name = outPath + '/' + r'_TwoCategoriesSVM.tif'\n",
    "            document.add_picture(img_name, width=Inches(5.0))\n",
    "        # KNN\n",
    "        t12 = document.add_heading(u'11.混淆矩阵：KNN测试结果', 2)\n",
    "        table12 = document.add_table(rows=1, cols=2, style='Table Grid')\n",
    "        table12.style.font.name = u'宋体'\n",
    "        table12.cell(0, 0).text = \"KNN测试准确率\"\n",
    "        table12.cell(0, 1).text = str(\"%.2f%%\" % (scoreKNN * 100))\n",
    "        img_name = outPath + '/' + r'_KNN.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # KNN_ROC_AUC\n",
    "        t13 = document.add_heading(u'10.ROC评估：KNN的ROC曲线及AUC值', 2)\n",
    "        table13 = document.add_table(rows=3, cols=2, style='Table Grid')\n",
    "        table13.style.font.name = u'宋体'\n",
    "        table13.cell(0, 0).text = \"macro-average ROC curve\"\n",
    "        table13.cell(0, 1).text = \"类别维度，n个类别ROC曲线均值\"\n",
    "        table13.cell(1, 0).text = \"micro-average ROC curve\"\n",
    "        table13.cell(1, 1).text = \"样本维度，计算所有测试样本ROC曲线均值\"\n",
    "        table13.cell(2, 0).text = \"ROC curve of class * \"\n",
    "        table13.cell(2, 1).text = \"类别 * 与KNN分类器预测概率绘制的ROC曲线\"\n",
    "        img_name = outPath + '/' + r'_KNN_ROC.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # KNN两类-四类\n",
    "        if filesNumInter <= 4:\n",
    "            t101 = document.add_heading(u'11(2).最佳回归线散点图：KNN测试结果', 2)\n",
    "            img_name = outPath + '/' + r'_TwoCategoriesKNN.tif'\n",
    "            document.add_picture(img_name, width=Inches(5.0))\n",
    "        # LDA\n",
    "        t14 = document.add_heading(u'12.混淆矩阵：LDA测试结果', 2)\n",
    "        table14 = document.add_table(rows=1, cols=2, style='Table Grid')\n",
    "        table14.style.font.name = u'宋体'\n",
    "        table14.cell(0, 0).text = \"测试准确率\"\n",
    "        table14.cell(0, 1).text = str(\"%.2f%%\" % (scoreLDA * 100))\n",
    "        img_name = outPath + '/' + r'_LDA.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # LDA_ROC_AUC\n",
    "        t15 = document.add_heading(u'10.ROC评估：LDA的ROC曲线及AUC值', 2)\n",
    "        table15 = document.add_table(rows=3, cols=2, style='Table Grid')\n",
    "        table15.style.font.name = u'宋体'\n",
    "        table15.cell(0, 0).text = \"macro-average ROC curve\"\n",
    "        table15.cell(0, 1).text = \"类别维度，n个类别ROC曲线均值\"\n",
    "        table15.cell(1, 0).text = \"micro-average ROC curve\"\n",
    "        table15.cell(1, 1).text = \"样本维度，计算所有测试样本ROC曲线均值\"\n",
    "        table15.cell(2, 0).text = \"ROC curve of class * \"\n",
    "        table15.cell(2, 1).text = \"类别 * 与LDA分类器预测概率绘制的ROC曲线\"\n",
    "        img_name = outPath + '/' + r'_LDA_ROC.tif'\n",
    "        document.add_picture(img_name, width=Inches(5.0))\n",
    "        # LDA两类-四类\n",
    "        if filesNumInter <= 4:\n",
    "            t101 = document.add_heading(u'12(2).最佳回归线散点图：LDA测试结果', 2)\n",
    "            img_name = outPath + '/' + r'_TwoCategoriesLDA.tif'\n",
    "            document.add_picture(img_name, width=Inches(5.0))\n",
    "\n",
    "        document.save(outPath + '/' + r\"HookeReport_Multifile.docx\")\n",
    "        # 保存文档\n",
    "        # ********************************************************************************************************************删除文件夹下所有tif图片\n",
    "\n",
    "        # del_files(outPath)\n",
    "\n",
    "        return 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ed0095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # # #\n",
    "# 调用测试\n",
    "# 调用测试\n",
    "#dataPath2 = r'./结果/4/输入数据/12-基于SVM模型的本次结果的预测-阴性对照/mapping10'\n",
    "dataPath = r'./data/消化道肿瘤-插值后'\n",
    "outPath =r'./result/消化道肿瘤-插值后'\n",
    "# 截谱\n",
    "startSpec = 80\n",
    "# endSpec = 0\n",
    "endSpec = 0\n",
    "# 前处理参数\n",
    "snrNoiseindex = 20\n",
    "# 从尾部选取长度，作为静默区\n",
    "\n",
    "delSNRmin = 0\n",
    "# 删除光谱信噪比阈值\n",
    "delSNRmax = 10000\n",
    "# 删除光谱信噪比阈值\n",
    "\n",
    "# 批处理参数\n",
    "# 2.1 宇宙射线阈值\n",
    "filterSize = 5\n",
    "# 整型\n",
    "dynamicFactor = 4.5\n",
    "# 浮点型\n",
    "# 2.2 S-G滤波\n",
    "winSG = 5\n",
    "nSG = 3\n",
    "lambdaAirPls = 100\n",
    "itermaxAirPls = 15\n",
    "# 更保型\n",
    "#lambdaAirPls = 100\n",
    "# itermaxAirPls = 15\n",
    "pltInter = 5\n",
    "# 画光谱数量\n",
    "processLinewidth = 0.5\n",
    "# 线宽\n",
    "\n",
    "# hca参数\n",
    "distanceMethod = 0\n",
    "# 层次聚类距离度量选用的方法,可选：'centroid','weighted','average'\n",
    "# hcaColorThreshold = 10\n",
    "# # color_threshold= 8 设置颜色高度阈值，用于控制决策树和PCA的颜色分类\n",
    "hcaLabelsize = 8\n",
    "# 标号大小\n",
    "\n",
    "# pca参数,和tsne共用参数\n",
    "pcaNcomponents = 2\n",
    "# 主成分个数，大于等于2\n",
    "pca_col1 = 0\n",
    "# 第一维，小于主成分个数\n",
    "pca_col2 = 1\n",
    "# 第二维，小于主成分个数\n",
    "pcaSize = 30\n",
    "# pca标签大小\n",
    "pcaAlpha = 1\n",
    "###############################################################20220301改的，之后改回来#pcaAlpha = 0.8\n",
    "# pca透明度\n",
    "\n",
    "# tSNE参数\n",
    "nDim = 2\n",
    "# int, 空间的维度\n",
    "nPerplex = 8\n",
    "# 数据集越大，需要参数值越大，建议值位5 - 50\n",
    "learnRate = 200\n",
    "# float,学习率，建议取值为10.0-1000.0\n",
    "nInter = 1000\n",
    "# int, 最大迭代次数\n",
    "nGrad = 1e-06\n",
    "# float,如果梯度低于该值，则停止算法\n",
    "\n",
    "# SVM参数\n",
    "svmTestSize = 0.2\n",
    "# 测试数据占全部数据比例\n",
    "svmC = 0.8\n",
    "# 错误项的惩罚系数,范围[0.5-1],C越大分类效果越好，但有可能会过拟合（defaul C = 1）\n",
    "xKernel = 0\n",
    "# str参数,算法中采用的核函数类型,可选'linear'、'poly'、'rbf'、'sigmoid'、'precomputed'，默认为rbf\n",
    "clusters_N = 4\n",
    "# 聚类堆数\n",
    "oneclickReport(dataPath, outPath, startSpec, endSpec, snrNoiseindex, delSNRmin, delSNRmax, filterSize, dynamicFactor,\n",
    "               winSG, nSG, lambdaAirPls,\n",
    "               itermaxAirPls, pltInter, processLinewidth, distanceMethod, hcaLabelsize, pcaNcomponents,\n",
    "               pca_col1, pca_col2, pcaSize, pcaAlpha, nDim, nPerplex, learnRate, nInter, nGrad, svmTestSize,\n",
    "               svmC, xKernel, clusters_N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8c6b363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)  #注意是双下划线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e91024",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cfa8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
